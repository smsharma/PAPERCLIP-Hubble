
\documentclass[10pt]{article} % For LaTeX2e
\usepackage[preprint]{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{url}
\usepackage{graphicx}
\usepackage{array}
\usepackage{booktabs}
\usepackage{lipsum}

\definecolor{xlinkcolor}{rgb}{0.7752941176470588, 0.22078431372549023, 0.2262745098039215}

\usepackage[
pdfnewwindow=true,      % links in new window
colorlinks=true,    % false: boxed links; true: colored links
linkcolor=xlinkcolor,     % color of internal links
citecolor=xlinkcolor,     % color of links to bibliography
filecolor=xlinkcolor,  % color of file links
urlcolor=xlinkcolor,      % color of external links
final=true,
]{hyperref}



\title{\textsc{HubbleCLIP}: Associating Astronomical Observations and Natural Language with Multi-Modal Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Siddharth Mishra-Sharma \email \href{mailto:smsharma@mit.edu}{smsharma@mit.edu} \\
      \addr The NSF AI Institute for Artificial Intelligence and Fundamental Interactions\\
      Center for Theoretical Physics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA \\
      Department of Physics, Harvard University, Cambridge, MA 02138, USA
      \AND
      \name Yiding Song \email \href{mailto:ydsong@mit.edu}{ydsong@mit.edu} \\
      \addr The NSF AI Institute for Artificial Intelligence and Fundamental Interactions\\
      Department of Physics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA \\
      \AND
      \name Jesse Thaler \email \href{mailto:jthaler@mit.edu}{jthaler@mit.edu} \\
      \addr The NSF AI Institute for Artificial Intelligence and Fundamental Interactions\\
      Center for Theoretical Physics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA \\
}
% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version

\newcommand{\datafolder}[1]{\def\thedatafolder{#1}}


\begin{document}


\maketitle

\begin{abstract}
We present a multi-modal model which associates astronomical observations imaged by the \emph{Hubble} Space Telescope (HST) with natural language. The model is fine-tuned from a base CLIP model using summarized proposal abstracts corresponding to HST observations. We show that the model embodies a meaningful joint representation between observations and text through experiments targeting observation retrieval (i.e., retrieving most relevant set of observations using natural language queries) and description retrieval (i.e., querying the astrophysical object classes and science use cases most relevant to a given observation). The model demonstrates the potential for using generalist rather than task-specific models for astrophysics research, in particular by leveraging text as an interface.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Machine learning (ML) is starting to have a significant impact on the sciences, with astrophysics being no exception. Machine learning methods have demonstrated promise when applied to every part of the astrophysics pipeline, from instrument design, to data acquisition, to its analysis. Until recently, most applications of ML within astrophysics have focused on replacing or augmenting traditional techniques with ML counterparts in order to improve performance on specific tasks.

The \emph{Foundation Model} paradigm, in contrast, seeks to develop generalist models which can be deployed on a wide range of tasks. The paradigm has been highly successful in domains like computer vision and natural language processing, as demonstrated by the widespread adoption of tools like CLIP, ChatGPT, Dall-E, and Stable Diffusion. These models are typically pre-trained on massive amounts of unlabeled data using self-supervised learning techniques, enabling them to learn powerful representations which can be optionally fine-tuned to address domain-specific tasks. At the heart of the paradigm lies the triumph of scale -- scaling up model size, dataset size, and compute. However, foundation models can often be fine-tuned using only small amounts of domain-specific data, increasing their usefulness when applied to those specific domains.

There is considerable interest in developing custom foundation models for the sciences, with astrophysics being ripe for such an effort due to several reasons. The first is the availability of large amounts of publicly-available data as a contingency of publicy-funded data-taking efforts. The second is the multi-modality inherent astrophysical observations, with different types of data (e.g., images, spectra, light curves, textual descriptions) often available for each observation. This multi-modality was recently exploited to train \textsc{AstroCLIP} -- a joint representation between multi-band images and optical spectra from the Dark Energy Spectroscopic Instrument (DESI). \textsc{AstroLLaMA} is another recent effort to fine-tune a publicly-available model (Llama-2) on astrophysics-specific textual data from the arXiv.

The success of the foundation model paradigm partly relies on the ability to leverage text as a \emph{universal interface}. In this work, we take this outlook and train a joint representation between observations taken by the \emph{Hubble} Space Telescope (HST) and natural language. We do so by using the associations between observation proposals and corresponding downstream observations. We show that fine-tuning a CLIP (Contrastive Language-Image Pre-training) model on this data enables learning meaningful joint representations.

The paper is organized as follows. In Sec.~\ref{sec:dataset}, we describe the dataset used in this work, including its curation and processing. In Sec.~\ref{sec:methodology}, we describe the methodology used to train and evaluate the model. In Sec.~\ref{sec:results}, we present the results of our experiments on image and text retrieval tasks. We discuss future prospects and conclude in Sec.~\ref{sec:conclusion}.

\section{Dataset and Processing}
\label{sec:dataset}

\subsection{Summarization via guided generation}
\label{sec:summarization}

\section{Methodology}
\label{sec:methodology}

\subsection{Language-Image Pre-training}
\label{sec:pretraining}

\subsection{Pre-trained CLIP Model}
\label{sec:clip}

\subsection{Fine-tuning Objectives}
\label{sec:finetuning}

\section{Results and Discussion}
\label{sec:results}

\subsection{Fine-tuned Retrieval Accuracy}
\label{sec:retrieval_acc}

\subsection{`Zero-shot' Hypothesis and Object Retrieval}
\label{sec:zero_shot}

\subsection{Text-to-Image Retrieval}
\label{sec:tti}

\section{Conclusion}
\label{sec:conclusion}


\cite{Hinton06}

% \begin{figure*}[!h]
% \includegraphics[width=0.95\textwidth]{example-image-a}
% \caption{Overview of dataset}
% \label{fig:dataset}
% \end{figure*}


\datafolder{./plots/data/} % Set your folder path here

\begin{table}[h!]
      \centering
      \begin{tabular}{m{0.21\textwidth} p{1.9cm} p{1.9cm} m{8cm}}
          \toprule
          \centering \bfseries Image & \centering \bfseries Obs. cycle & \centering \bfseries Prop. ID & \centering \bfseries Summarized abstract \tabularnewline
          \midrule
          \centering \includegraphics[width=0.2\textwidth]{\thedatafolder/img_0.pdf} & \centering \input{\thedatafolder/cycle_0.txt} & \centering \input{\thedatafolder/id_0.txt} &  {\scriptsize \input{\thedatafolder/sum_0.txt}} \tabularnewline
          \midrule
          \centering \includegraphics[width=0.2\textwidth]{\thedatafolder/img_1.pdf} & \centering \input{\thedatafolder/cycle_1.txt} & \centering \input{\thedatafolder/id_1.txt} &  {\scriptsize \input{\thedatafolder/sum_1.txt}} \tabularnewline
          \midrule
          \centering \includegraphics[width=0.2\textwidth]{\thedatafolder/img_2.pdf} & \centering \input{\thedatafolder/cycle_2.txt} & \centering \input{\thedatafolder/id_2.txt} &  {\scriptsize \input{\thedatafolder/sum_2.txt}} \tabularnewline
          \midrule
          \centering \includegraphics[width=0.2\textwidth]{\thedatafolder/img_3.pdf} & \centering \input{\thedatafolder/cycle_3.txt} & \centering \input{\thedatafolder/id_3.txt} &  {\scriptsize \input{\thedatafolder/sum_3.txt}} \tabularnewline
          \bottomrule
      \end{tabular}
      \caption{Data overview}
  \end{table}
  

\begin{figure*}[!h]
      \includegraphics[width=0.95\textwidth]{plots/val_metrics.pdf}
      \caption{Retrieval accuracy}
      \label{fig:retrieval_acc}
      \end{figure*}
      

\begin{figure*}[!h]
\includegraphics[width=0.95\textwidth]{plots/retrieval_acc.pdf}
\caption{Retrieval accuracy}
\label{fig:retrieval_acc}
\end{figure*}

\begin{figure*}[!h]
\includegraphics[width=0.95\textwidth]{plots/itt.pdf}
\caption{Retrieval accuracy}
\label{fig:itt}
\end{figure*}

\begin{figure*}[!h]
\includegraphics[width=0.95\textwidth]{plots/tti_base.pdf}
\caption{Retrieval accuracy}
\label{fig:tti}
\end{figure*}

\begin{figure*}[!h]
\includegraphics[width=0.95\textwidth]{plots/tti.pdf}
\caption{Retrieval accuracy}
\label{fig:tti_base}
\end{figure*}

\subsubsection*{Broader Impact Statement}
In this optional section, TMLR encourages authors to discuss possible repercussions of their work,
notably any potential negative impact that a user of this research should be aware of. 
Authors should consult the TMLR Ethics Guidelines available on the TMLR website
for guidance on how to approach this subject.

\subsubsection*{Author Contributions}
If you'd like to, you may include a section for author contributions as is done
in many journals. This is optional and at the discretion of the authors. Only add
this information once your submission is accepted and deanonymized. 

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.
Only add this information once your submission is accepted and deanonymized. 

\bibliography{main}
\bibliographystyle{tmlr}

\appendix
\section{Appendix}
You may include other additional sections here.

\end{document}
