{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a099dd4-cd60-4df7-9fbf-2136e00cfa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 16:56:10.043841: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-09 16:56:10.044581: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-09 16:56:10.135589: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from transformers import FlaxCLIPModel, AutoProcessor, AutoTokenizer\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca87e85f-3e24-4e66-ab8a-1cf4af3c44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlaxCLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc3a1bfa-d17d-49c5-a009-eb7b82b56120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "from matplotlib import cm\n",
    "cmap = matplotlib.colormaps.get_cmap('viridis_r')\n",
    "\n",
    "# Ignore warning\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.MatplotlibDeprecationWarning)\n",
    "\n",
    "# Get plot params\n",
    "\n",
    "from plot_params import params\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "# Set default colors to load at will\n",
    "cols_default = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8e92d8-d6c5-4331-a341-ebeb05d54831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from ml_collections.config_dict import ConfigDict\n",
    "\n",
    "logging_dir = '/n/holystore01/LABS/iaifi_lab/Users/smsharma/multimodal-data/logging/proposals/'\n",
    "run_name = 'glistening-kumquat-123'\n",
    "\n",
    "config_file = \"{}/{}/config.yaml\".format(logging_dir, run_name)\n",
    "\n",
    "with open(config_file, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "config = ConfigDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d83087c-1986-44ff-8953-694c3a2a31fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax \n",
    "from flax.training import checkpoints, common_utils, train_state\n",
    "\n",
    "import flax\n",
    "\n",
    "replicate = flax.jax_utils.replicate\n",
    "unreplicate = flax.jax_utils.unreplicate\n",
    "\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=1e-4,\n",
    "    warmup_steps=5_000,\n",
    "    decay_steps=100_000,\n",
    ")\n",
    "\n",
    "tx = optax.adamw(learning_rate=schedule, weight_decay=1e-4)\n",
    "state = train_state.TrainState.create(apply_fn=model.__call__, params=model.params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4806fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "model_large = FlaxCLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor_large = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "state_large = train_state.TrainState.create(apply_fn=model_large.__call__, params=model_large.params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "238f857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbax.checkpoint as ocp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a06d988-f08e-4136-8264-e7e20859cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \n",
    "    norm_vec1 = np.linalg.norm(vec1, axis=-1,)\n",
    "    norm_vec2 = np.linalg.norm(vec2, axis=-1,)\n",
    "        \n",
    "    return np.dot(vec1, vec2) / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bbb923-23ed-4f9e-ae99-4188dbb43d42",
   "metadata": {},
   "source": [
    "## Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55eb3de5-f20d-4db1-91bf-1fe8e3773f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.text_utils import process_truncate_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c783dec2-b2f3-454c-8782-576da5bf6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from utils.dataset_utils import make_dataloader, create_input_iter\n",
    "from dm_pix import center_crop, random_crop, rotate, random_flip_up_down, random_flip_left_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3d12481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the dataframes\n",
    "sum1_filename = \"../data/summary_sum1_v3.csv\"\n",
    "sum1_df = pd.read_csv(sum1_filename)\n",
    "\n",
    "summaries_filename = \"../data/summary_v2.csv\"\n",
    "summaries_df = pd.read_csv(summaries_filename)\n",
    "\n",
    "# Merging the dataframes\n",
    "sum_merged = pd.merge(summaries_df, sum1_df, on='proposal_id')\n",
    "\n",
    "# Function to process each caption\n",
    "def get_objects_phenomena(caption):\n",
    "    first_part = caption.split(';')[0]\n",
    "    match = sum_merged[sum_merged['objects_phenomena_x'] == first_part]['objects_phenomena_y']\n",
    "    return match.values[0] if not (match.empty or pd.isna(match.values[0])) else \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d92a1d51-08ae-4fd6-9df8-06b91238cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from einops import rearrange\n",
    "import numpy as onp\n",
    "\n",
    "    \n",
    "def get_features_ds(state, model, processor, ds, truncate=False, use_sum1=False, randomize_objects=False, num_rnd=3):\n",
    "\n",
    "    @partial(jax.pmap, axis_name=\"batch\")\n",
    "    def get_features(state, input_ids, pixel_values, attention_mask):\n",
    "        captions_feat = model.get_text_features(input_ids, attention_mask, params=state.params)\n",
    "        images_feat = model.get_image_features(pixel_values, params=state.params)\n",
    "        return images_feat, captions_feat\n",
    "\n",
    "    batches = iter(ds)\n",
    "\n",
    "    image_feat_stack = []\n",
    "    images_stack = []\n",
    "    captions_stack = []\n",
    "    captions_feat_stack = []\n",
    "    \n",
    "    num_local_devices = jax.local_device_count()\n",
    "    replicate = flax.jax_utils.replicate\n",
    "    \n",
    "    total_batches = sum(1 for _ in ds) - 1\n",
    "    current_batch = 0\n",
    "\n",
    "    for (images, captions) in tqdm(batches, total=total_batches):\n",
    "        if current_batch == total_batches - 1:\n",
    "            break\n",
    "    \n",
    "        images = np.array(images)\n",
    "\n",
    "        if truncate:\n",
    "            rng_seed = onp.random.randint(99999)\n",
    "            captions = process_truncate_captions(captions, jax.random.PRNGKey(rng_seed), max_length_words=config.data.max_length_words)\n",
    "        else:\n",
    "            captions = captions.numpy().tolist()\n",
    "            captions = [c.decode('utf-8') for c in captions]\n",
    "\n",
    "        if randomize_objects:\n",
    "            captions_rnd = []\n",
    "            for caption in captions:\n",
    "                list_rnd = onp.random.choice(\", \".join(caption.split(';')).split(', '), num_rnd)\n",
    "                # Join into comma separated string\n",
    "                list_rnd = \", \".join(list_rnd)\n",
    "                captions_rnd.append(list_rnd)\n",
    "    \n",
    "            captions = captions_rnd\n",
    "        \n",
    "        if use_sum1:\n",
    "            captions_sum1 = []\n",
    "            for caption in captions:\n",
    "                sum1 = sum_merged[sum_merged['objects_phenomena_x'] == caption.split(';')[0]]['objects_phenomena_y'].values[0]\n",
    "                if sum1 is np.nan:\n",
    "                    sum1 = \"None\"\n",
    "                captions_sum1 += [sum1]\n",
    "                \n",
    "            captions_stack += captions_sum1\n",
    "        else:\n",
    "            captions_stack += captions\n",
    "\n",
    "        images_stack.append(images)\n",
    "\n",
    "        rng_eval = jax.random.PRNGKey(onp.random.randint(99999))\n",
    "        \n",
    "        # Rotation angles in rad\n",
    "        rot_angles_90 = np.array([0.0, np.pi / 2, np.pi, 3 * np.pi / 2])\n",
    "\n",
    "        # Rotations\n",
    "        rng_eval, _ = jax.random.split(rng_eval)\n",
    "        # rotation_angles = jax.random.uniform(rng_eval, shape=(images.shape[0],)) * 2 * np.pi  # Angles in radians\n",
    "        rotation_angles = jax.random.choice(\n",
    "            rng_eval, rot_angles_90, shape=(images.shape[0],)\n",
    "        )  # Angles in radians\n",
    "\n",
    "        images = jax.vmap(partial(rotate, mode='constant', cval=1.))(images, rotation_angles)\n",
    "        \n",
    "        # Flips\n",
    "        rng_eval, _ = jax.random.split(rng_eval)\n",
    "        images = jax.vmap(partial(random_flip_up_down, key=rng_eval))(image=images)\n",
    "\n",
    "        rng_eval, _ = jax.random.split(rng_eval)\n",
    "        images = jax.vmap(partial(random_flip_left_right, key=rng_eval))(image=images)\n",
    "\n",
    "        images = jax.vmap(random_crop, in_axes=(None,0,None))(rng_eval, images, (model.config.vision_config.image_size, model.config.vision_config.image_size, 3))\n",
    "\n",
    "        input = processor(text=captions_sum1 if use_sum1 else captions, images=(images * 255.).astype(np.uint8), return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=77)\n",
    "    \n",
    "        batch = jax.tree_map(lambda x: np.split(x, num_local_devices, axis=0), input.data)\n",
    "    \n",
    "        image_feat, captions_feat = get_features(replicate(state), np.array(batch[\"input_ids\"]), np.array(batch[\"pixel_values\"]), np.array(batch[\"attention_mask\"]))\n",
    "\n",
    "        image_feat = rearrange(image_feat, \"d b e -> (d b) e\")\n",
    "        captions_feat = rearrange(captions_feat, \"d b e -> (d b) e\")\n",
    "\n",
    "        captions_feat_stack.append(captions_feat)\n",
    "        image_feat_stack.append(image_feat)\n",
    "\n",
    "        current_batch += 1\n",
    "\n",
    "    return image_feat_stack, captions_feat_stack, images_stack, captions_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a331940-9759-4b2c-94b6-d2372ce36587",
   "metadata": {},
   "source": [
    "## Multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef189285-df3e-4360-98b1-80a8868bd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc(image_feat, captions_feat):\n",
    "    \n",
    "    roc_mat = np.matmul(image_feat, captions_feat.T)\n",
    "    \n",
    "    correct_indices = np.arange(roc_mat.shape[0])[:, None]\n",
    "    top_indices = np.argsort(roc_mat, axis=-1)\n",
    "    \n",
    "    accuracy_list = []\n",
    "    for k in np.arange(1, roc_mat.shape[0], 100):\n",
    "        correct_in_top_k = np.any(top_indices[:, -k:] == correct_indices, axis=-1)\n",
    "        accuracy = np.mean(correct_in_top_k.astype(np.float32))\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    return accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "976345c4-c4a8-4d0e-9679-731b65312b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_eval_metric(text_embeds, image_embeds, k=[1, 5, 10, 20]):\n",
    "    \"\"\" Compute the top-k retrieval accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get shapes\n",
    "    bs = text_embeds.shape[0]\n",
    "    axis_size = jax.lax.psum(1, axis_name=\"batch\")\n",
    "\n",
    "    # Gather the embeddings from all devices\n",
    "    all_text_embeds = jax.lax.all_gather(text_embeds, axis_name=\"batch\").reshape(-1, text_embeds.shape[-1])\n",
    "    all_image_embeds = jax.lax.all_gather(image_embeds, axis_name=\"batch\").reshape(-1, image_embeds.shape[-1])\n",
    "\n",
    "    # Compute the full matrix of logitseval\n",
    "    all_logits = np.matmul(all_text_embeds, all_image_embeds.T)\n",
    "\n",
    "    # Compute the global top-k indices for the maximum k value\n",
    "    max_k = max(k)\n",
    "    top_k_indices = np.argsort(all_logits, axis=-1)[:, -max_k:]\n",
    "\n",
    "    # Compute the correct indices for each row\n",
    "    correct_indices = np.arange(bs * axis_size)[:, None]\n",
    "\n",
    "    metrics = {}\n",
    "    for current_k in k:\n",
    "        # Check if the correct image (diagonal) is in the current top-k for each text embedding\n",
    "        correct_in_top_k = np.any(top_k_indices[:, -current_k:] == correct_indices, axis=-1)\n",
    "        accuracy = np.mean(correct_in_top_k.astype(np.float32))\n",
    "        metrics[f\"top_{current_k}_accuracy\"] = accuracy\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "394863d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_labels = ['major-meadow-131',\n",
    "              'exalted-cosmos-130',\n",
    "              'magic-oath-132',\n",
    "              'glistening-kumquat-123',\n",
    "              \"base\"\n",
    "              ]\n",
    "\n",
    "run_legends = [r'Fine-tune (summaries), \\texttt{clip-vit-large-patch14}', \n",
    "               r'Fine-tune (summaries), $\\mathrm{LR}=10^{-6}$', \n",
    "               r'Fine-tune (summaries), cosine LR schedule', \n",
    "               'Fine-tune (summaries)', \n",
    "               'base'\n",
    "               ]\n",
    "\n",
    "\n",
    "data_type = [\"summary\", \"summary\", \"summary\",\"summary\",\"summary\",]\n",
    "use_sum1 = [False, False, False, False,False,]\n",
    "\n",
    "cols_list = [cols_default[0], cols_default[1], cols_default[5], cols_default[3], cols_default[4]]\n",
    "ls_list = ['-', '-', '-', '-', '-']\n",
    "\n",
    "model_is_large = [True, False, False, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "512f6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_lists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "404f1ac0-3e64-4d6b-8c24-8a3ad8510453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 30/31 [01:47<00:03,  3.59s/it]\n",
      " 97%|█████████▋| 30/31 [01:24<00:02,  2.81s/it]\n",
      " 97%|█████████▋| 30/31 [01:22<00:02,  2.77s/it]\n",
      " 97%|█████████▋| 30/31 [01:23<00:02,  2.79s/it]\n",
      " 97%|█████████▋| 30/31 [01:22<00:02,  2.75s/it]\n",
      "100%|██████████| 5/5 [07:51<00:00, 94.31s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx, run_name in enumerate(tqdm(run_labels[:])):\n",
    "\n",
    "    files = tf.io.gfile.glob(f\"/n/holyscratch01/iaifi_lab/smsharma/hubble_data/tfrecords_v5/*val*.tfrecord\")\n",
    "    ds = make_dataloader(files, batch_size=100, seed=324, split=\"val\", shuffle=True, caption_type=data_type[idx])\n",
    "    \n",
    "    if run_name == \"base\":\n",
    "        restored_state = state  \n",
    "    else:\n",
    "        ckpt_dir = \"{}/{}\".format(logging_dir, run_name)  # Load SLURM run\n",
    "        \n",
    "        best_fn = lambda metrics: metrics[f\"val/loss\"]\n",
    "        \n",
    "        mgr_options = ocp.CheckpointManagerOptions(step_prefix=f'step', best_fn=best_fn, best_mode='min', create=False)\n",
    "\n",
    "        ckpt_mgr_load_ckpt = ocp.CheckpointManager(\n",
    "            f\"{ckpt_dir}/ckpts/\",\n",
    "            options=mgr_options,\n",
    "        )\n",
    "\n",
    "        if model_is_large[idx]:\n",
    "            restored_state = ckpt_mgr_load_ckpt.restore(\n",
    "                ckpt_mgr_load_ckpt.latest_step(),\n",
    "                args=ocp.args.StandardRestore(state_large)\n",
    "            )\n",
    "\n",
    "            if state_large is restored_state:\n",
    "                raise FileNotFoundError(f\"Did not load checkpoint correctly\")\n",
    "\n",
    "        else:\n",
    "            restored_state = ckpt_mgr_load_ckpt.restore(\n",
    "                ckpt_mgr_load_ckpt.latest_step(),\n",
    "                args=ocp.args.StandardRestore(state)\n",
    "            )\n",
    "        \n",
    "            if state is restored_state:\n",
    "                raise FileNotFoundError(f\"Did not load checkpoint correctly\")\n",
    "\n",
    "    image_feat_stack, captions_feat_stack, _, _ = get_features_ds(restored_state, \n",
    "                                                                  model_large if model_is_large[idx] else model, \n",
    "                                                                  processor_large if model_is_large[idx] else processor, \n",
    "                                                                  ds, truncate=data_type[idx] == \"abstract\", use_sum1=use_sum1[idx], randomize_objects=False, num_rnd=3)\n",
    "    \n",
    "    accuracy_list = get_roc(np.vstack(image_feat_stack), np.vstack(captions_feat_stack))\n",
    "    accuracy_lists.append(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e07245ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_labels = ['major-meadow-131',\n",
    "              'exalted-cosmos-130',\n",
    "              'magic-oath-132',\n",
    "              'glistening-kumquat-123',\n",
    "              \"base\"\n",
    "              ]\n",
    "\n",
    "run_legends = [r'Fine-tune larger CLIP model', \n",
    "               r'$\\mathrm{LR}=10^{-6}$', \n",
    "               r'Cosine LR schedule', \n",
    "               'Fine-tune (summaries)', \n",
    "               'Base'\n",
    "               ]\n",
    "\n",
    "\n",
    "data_type = [\"summary\", \"summary\", \"summary\",\"summary\",\"summary\",]\n",
    "use_sum1 = [False, False, False, False,False,]\n",
    "\n",
    "cols_list = [cols_default[0], cols_default[2],cols_default[2],  cols_default[0], cols_default[3], ]\n",
    "ls_list = [':', '--', '-', '-', '-']\n",
    "\n",
    "model_is_large = [True, False, False, False, False]\n",
    "alpha_list = [1.0, 1.0, 0.5, 1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ce36de59-cc1f-42fc-b663-6ac59802448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 6.2))\n",
    "\n",
    "for idx, accuracy_list in enumerate(accuracy_lists):\n",
    "    ax.plot(np.linspace(0., 1., len(accuracy_list)), accuracy_list, label=run_legends[idx], color=cols_list[idx], ls=ls_list[idx], alpha=alpha_list[idx])\n",
    "\n",
    "ax.plot([0, 1], [0, 1], ls='--', alpha=0.5, color='grey')\n",
    "    \n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "ax.set_xlabel(\"Retrieval fraction\")\n",
    "ax.set_ylabel(\"Retrieval accuracy\")\n",
    "\n",
    "ax.legend(frameon=False, framealpha=0.9)\n",
    "ax.set_title(r\"\\textbf{Ablation Tests (Using Summarized Abstracts)}\", y=1.01)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../paper/plots/retrieval_app.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6b3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
