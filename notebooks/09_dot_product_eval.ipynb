{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a099dd4-cd60-4df7-9fbf-2136e00cfa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 20:21:30.335565: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-16 20:21:30.335616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-16 20:21:30.336983: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from transformers import FlaxCLIPModel, AutoProcessor, AutoTokenizer\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca87e85f-3e24-4e66-ab8a-1cf4af3c44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlaxCLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc3a1bfa-d17d-49c5-a009-eb7b82b56120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "from matplotlib import cm\n",
    "cmap = matplotlib.colormaps.get_cmap('viridis_r')\n",
    "\n",
    "# Ignore warning\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.MatplotlibDeprecationWarning)\n",
    "\n",
    "# Get plot params\n",
    "\n",
    "from plot_params import params\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "# Set default colors to load at will\n",
    "cols_default = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8e92d8-d6c5-4331-a341-ebeb05d54831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from ml_collections.config_dict import ConfigDict\n",
    "\n",
    "logging_dir = '../logging/proposals/'\n",
    "run_name = 'polar-hill-76'\n",
    "\n",
    "config_file = \"{}/{}/config.yaml\".format(logging_dir, run_name)\n",
    "\n",
    "with open(config_file, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "config = ConfigDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d83087c-1986-44ff-8953-694c3a2a31fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax \n",
    "from flax.training import checkpoints, common_utils, train_state\n",
    "\n",
    "import flax\n",
    "\n",
    "replicate = flax.jax_utils.replicate\n",
    "unreplicate = flax.jax_utils.unreplicate\n",
    "\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=1e-4,\n",
    "    warmup_steps=5_000,\n",
    "    decay_steps=100_000,\n",
    ")\n",
    "\n",
    "tx = optax.adamw(learning_rate=schedule, weight_decay=1e-4)\n",
    "state = train_state.TrainState.create(apply_fn=model.__call__, params=model.params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846c9fae-a277-4102-9545-4bb93968991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbax\n",
    "\n",
    "ckpt_dir = \"{}/{}\".format(logging_dir, run_name)  # Load SLURM run\n",
    "\n",
    "best_fn = lambda metrics: metrics[f\"val/top_10_accuracy\"]\n",
    "\n",
    "mgr_options = orbax.checkpoint.CheckpointManagerOptions(step_prefix=f'step', best_fn=best_fn, best_mode='min', create=False)\n",
    "ckpt_mgr = orbax.checkpoint.CheckpointManager(f\"{ckpt_dir}/ckpts/\", orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler()), mgr_options)\n",
    "\n",
    "restore_args = flax.training.orbax_utils.restore_args_from_target(state, mesh=None)\n",
    "restored_state = ckpt_mgr.restore(ckpt_mgr.latest_step(), items=state, restore_kwargs={'restore_args': restore_args})\n",
    "\n",
    "if state is restored_state:\n",
    "    raise FileNotFoundError(f\"Did not load checkpoint correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a06d988-f08e-4136-8264-e7e20859cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \n",
    "    norm_vec1 = np.linalg.norm(vec1, axis=-1,)\n",
    "    norm_vec2 = np.linalg.norm(vec2, axis=-1,)\n",
    "        \n",
    "    return np.dot(vec1, vec2) / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bbb923-23ed-4f9e-ae99-4188dbb43d42",
   "metadata": {},
   "source": [
    "## Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55eb3de5-f20d-4db1-91bf-1fe8e3773f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.text_utils import process_truncate_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c783dec2-b2f3-454c-8782-576da5bf6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from utils.dataset_utils import make_dataloader, create_input_iter\n",
    "from dm_pix import center_crop, random_crop, rotate, random_flip_up_down, random_flip_left_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d92a1d51-08ae-4fd6-9df8-06b91238cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from einops import rearrange\n",
    "import numpy as onp\n",
    "\n",
    "@partial(jax.pmap, axis_name=\"batch\")\n",
    "def get_features(state, input_ids, pixel_values, attention_mask):\n",
    "    captions_feat = model.get_text_features(input_ids, attention_mask, params=state.params)\n",
    "    images_feat = model.get_image_features(pixel_values, params=state.params)\n",
    "    return images_feat, captions_feat\n",
    "    \n",
    "def get_features_ds(state, ds, truncate=False, use_sum1=False, randomize_objects=False):\n",
    "\n",
    "    batches = iter(ds)\n",
    "\n",
    "    image_feat_stack = []\n",
    "    images_stack = []\n",
    "    captions_stack = []\n",
    "    captions_feat_stack = []\n",
    "    \n",
    "    num_local_devices = jax.local_device_count()\n",
    "    replicate = flax.jax_utils.replicate\n",
    "    \n",
    "    total_batches = sum(1 for _ in ds) - 1\n",
    "    current_batch = 0\n",
    "\n",
    "    for (images, captions) in tqdm(batches, total=total_batches):\n",
    "        if current_batch == total_batches - 1:\n",
    "            break\n",
    "    \n",
    "        images = np.array(images)\n",
    "\n",
    "        if truncate:\n",
    "            captions = process_truncate_captions(captions, jax.random.PRNGKey(onp.random.randint(99999)), max_length_words=config.data.max_length_words)\n",
    "        else:\n",
    "            captions = captions.numpy().tolist()\n",
    "            captions = [c.decode('utf-8') for c in captions]\n",
    "\n",
    "        if randomize_objects:\n",
    "            captions_rnd = []\n",
    "            for caption in captions:\n",
    "                captions_rnd.append(onp.random.choice(\", \".join(caption.split(';')).split(', ')))\n",
    "    \n",
    "            captions = captions_rnd\n",
    "        \n",
    "        if use_sum1:\n",
    "            captions_sum1 = []\n",
    "            for caption in captions:\n",
    "                sum1 = sum_merged[sum_merged['objects_phenomena_x'] == caption.split(';')[0]]['objects_phenomena_y'].values[0]\n",
    "                if sum1 is np.nan:\n",
    "                    sum1 = \"None\"\n",
    "                captions_sum1 += [sum1]\n",
    "                \n",
    "            captions_stack += captions_sum1\n",
    "        else:\n",
    "            captions_stack += captions\n",
    "\n",
    "        images_stack.append(images)\n",
    "\n",
    "        rng_eval = jax.random.PRNGKey(onp.random.randint(99999))\n",
    "        \n",
    "        # Rotations\n",
    "        rng_eval, _ = jax.random.split(rng_eval)\n",
    "        rotation_angles = jax.random.uniform(rng_eval, shape=(images.shape[0],)) * 2 * np.pi  # Angles in radians\n",
    "        images = jax.vmap(partial(rotate, mode='constant', cval=1.))(images, rotation_angles)\n",
    "        \n",
    "        # Flips\n",
    "        rng_eval, _ = jax.random.split(rng_eval)\n",
    "        images = jax.vmap(partial(random_flip_up_down, key=rng_eval))(image=images)\n",
    "\n",
    "        rng_eval, _ = jax.random.split(rng_eval)\n",
    "        images = jax.vmap(partial(random_flip_left_right, key=rng_eval))(image=images)\n",
    "\n",
    "        images = jax.vmap(random_crop, in_axes=(None,0,None))(rng_eval, images, (model.config.vision_config.image_size, model.config.vision_config.image_size, 3))\n",
    "\n",
    "        print(np.max(images))\n",
    "        input = processor(text=captions_sum1 if use_sum1 else captions, images=(images * 255.), return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=77)\n",
    "    \n",
    "        batch = jax.tree_map(lambda x: np.split(x, num_local_devices, axis=0), input.data)\n",
    "    \n",
    "        image_feat, captions_feat = get_features(replicate(state), np.array(batch[\"input_ids\"]), np.array(batch[\"pixel_values\"]), np.array(batch[\"attention_mask\"]))\n",
    "\n",
    "        image_feat = rearrange(image_feat, \"d b e -> (d b) e\")\n",
    "        captions_feat = rearrange(captions_feat, \"d b e -> (d b) e\")\n",
    "\n",
    "        captions_feat_stack.append(captions_feat)\n",
    "        image_feat_stack.append(image_feat)\n",
    "\n",
    "        current_batch += 1\n",
    "\n",
    "    return image_feat_stack, captions_feat_stack, images_stack, captions_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e336a92d-4f40-4840-a41a-eceae6edd554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████▋| 98/99 [01:02<00:00,  1.56it/s]\n",
      " 99%|█████████████████████████████▋| 98/99 [00:45<00:00,  2.17it/s]\n"
     ]
    }
   ],
   "source": [
    "files = tf.io.gfile.glob(f\"/n/holyscratch01/iaifi_lab/smsharma/hubble_data/tfrecords_v5/*val*.tfrecord\")\n",
    "ds = make_dataloader(files, batch_size=config.training.batch_size, seed=config.seed, split=\"val\", shuffle=False, caption_type=\"summary\")\n",
    "\n",
    "image_feat_stack, captions_feat_stack, images_stack, captions_stack = get_features_ds(restored_state, ds)\n",
    "image_base_feat_stack, captions_feat_base_stack, images_base_stack, captions_base_stack = get_features_ds(state, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd43e886-a1c9-4d89-b3c5-1c3899bc58bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 498.96x388.8 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-1, 1, 30)\n",
    "lw = 2\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7.7 * 0.9, 6 * 0.9))\n",
    "\n",
    "ax.hist(jax.vmap(cosine_similarity)(np.vstack(image_feat_stack), np.vstack(captions_feat_stack)[:]), histtype='step', label=\"Fine-tuned\", bins=bins, lw=lw)\n",
    "ax.hist(jax.vmap(cosine_similarity)(np.vstack(image_base_feat_stack), np.vstack(captions_feat_base_stack)), histtype='step', label=\"Base\", bins=bins, lw=lw)\n",
    "ax.hist(jax.vmap(cosine_similarity)(np.vstack(image_feat_stack), jax.random.permutation(jax.random.PRNGKey(42), np.vstack(captions_feat_stack)[:])), histtype='step', label=\"Fine-tuned, shuffled\", bins=bins, lw=lw, color=cols_default[0], ls='--', alpha=0.4)\n",
    "ax.hist(jax.vmap(cosine_similarity)(np.vstack(image_base_feat_stack), jax.random.permutation(jax.random.PRNGKey(42), np.vstack(captions_feat_base_stack)[:])), histtype='step', label=\"Base, shuffled\", bins=bins, lw=lw, color=cols_default[1], ls='--', alpha=0.4)\n",
    "\n",
    "ax.set_xlabel(\"$x_i\\cdot y_i$\")\n",
    "ax.legend()\n",
    "ax.set_title(r\"\\textbf{Cosine similarities}\", y=1.01)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"../paper/plots/sim_val.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c980d4d-038e-485e-9cd7-cd82636c39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = tf.io.gfile.glob(f\"/n/holyscratch01/iaifi_lab/smsharma/hubble_data/tfrecords_v5/*train_9.tfrecord\")\n",
    "# ds = make_dataloader(files, batch_size=config.training.batch_size, seed=config.seed, split=\"val\", shuffle=False, caption_type=\"summary\")\n",
    "\n",
    "# image_feat_train_stack, captions_feat_train_stack, images_train_stack, captions_train_stack = get_features_ds(restored_state, ds)\n",
    "# image_base_feat_train_stack, captions_feat_train_base_stack, images_train_base_stack, captions_train_base_stack = get_features_ds(state, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b924b36-f24f-43b5-868a-728d2bf4a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = np.linspace(-1, 1, 30)\n",
    "# lw = 2\n",
    "\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# ax[0].hist(jax.vmap(cosine_similarity)(np.vstack(image_feat_stack), np.vstack(captions_feat_stack)[:]), histtype='step', label=\"Fine-tuned\", bins=bins, lw=lw)\n",
    "# ax[0].hist(jax.vmap(cosine_similarity)(np.vstack(image_base_feat_stack), np.vstack(captions_feat_base_stack)), histtype='step', label=\"Base\", bins=bins, lw=lw)\n",
    "# ax[0].hist(jax.vmap(cosine_similarity)(np.vstack(image_feat_stack), jax.random.permutation(jax.random.PRNGKey(42), np.vstack(captions_feat_stack)[:])), histtype='step', label=\"Fine-tuned, shuffled\", bins=bins, lw=lw, color=cols_default[0], ls='--', alpha=0.4)\n",
    "# ax[0].hist(jax.vmap(cosine_similarity)(np.vstack(image_base_feat_stack), jax.random.permutation(jax.random.PRNGKey(42), np.vstack(captions_feat_base_stack)[:])), histtype='step', label=\"Base, shuffled\", bins=bins, lw=lw, color=cols_default[1], ls='--', alpha=0.4)\n",
    "\n",
    "# ax[0].set_xlabel(\"$x_i\\cdot y_i$\")\n",
    "# ax[0].legend()\n",
    "# ax[0].set_title(r\"\\textbf{Validation examples}\", y=1.01)\n",
    "\n",
    "# ax[1].hist(jax.vmap(cosine_similarity)(np.vstack(image_feat_train_stack), np.vstack(captions_feat_train_stack)[:]), histtype='step', label=\"Fine-tuned\", bins=bins, lw=lw)\n",
    "# ax[1].hist(jax.vmap(cosine_similarity)(np.vstack(image_feat_train_stack), jax.random.permutation(jax.random.PRNGKey(42), np.vstack(captions_feat_train_stack)[:])), histtype='step', label=\"Fine-tuned, shuffled\", bins=bins, lw=lw, color=cols_default[0], ls='--', alpha=0.4)\n",
    "# ax[1].hist(jax.vmap(cosine_similarity)(np.vstack(image_base_feat_train_stack), np.vstack(captions_feat_train_base_stack)), histtype='step', label=\"Base\", bins=bins, lw=lw)\n",
    "# ax[1].hist(jax.vmap(cosine_similarity)(np.vstack(image_base_feat_train_stack), jax.random.permutation(jax.random.PRNGKey(42), np.vstack(captions_feat_train_base_stack)[:])), histtype='step', label=\"Base, shuffled\", bins=bins, lw=lw, color=cols_default[1], ls='--', alpha=0.4)\n",
    "\n",
    "# ax[1].set_xlabel(\"$x_i\\cdot y_i$\")\n",
    "# ax[1].legend()\n",
    "# ax[1].set_title(r\"\\textbf{Training examples}\", y=1.01)\n",
    "\n",
    "# fig.suptitle(r\"\\textbf{Cosine Similarities}\", fontsize=20)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"../paper/plots/sim_valtrain.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85a2c0-e69f-4e64-9cf5-5da43b9e55fd",
   "metadata": {},
   "source": [
    "## Single-concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "304584be-1b85-4ee3-9d3d-32ffeb66986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the dataframes\n",
    "sum1_filename = \"../data/summary_sum1_v3.csv\"\n",
    "sum1_df = pd.read_csv(sum1_filename)\n",
    "\n",
    "summaries_filename = \"../data/summary_v2.csv\"\n",
    "summaries_df = pd.read_csv(summaries_filename)\n",
    "\n",
    "# Merging the dataframes\n",
    "sum_merged = pd.merge(summaries_df, sum1_df, on='proposal_id')\n",
    "\n",
    "# Function to process each caption\n",
    "def get_objects_phenomena(caption):\n",
    "    first_part = caption.split(';')[0]\n",
    "    match = sum_merged[sum_merged['objects_phenomena_x'] == first_part]['objects_phenomena_y']\n",
    "    return match.values[0] if not (match.empty or pd.isna(match.values[0])) else \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0e024d-5b0a-4300-abc4-f17e26832a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import orbax\n",
    "\n",
    "# run_name = \"pretty-aardvark-45\"\n",
    "\n",
    "# ckpt_dir = \"{}/{}\".format(logging_dir, run_name)  # Load SLURM run\n",
    "\n",
    "# best_fn = lambda metrics: metrics[f\"val/top_10_accuracy\"]\n",
    "\n",
    "# mgr_options = orbax.checkpoint.CheckpointManagerOptions(step_prefix=f'step', best_fn=best_fn, best_mode='min', create=False)\n",
    "# ckpt_mgr = orbax.checkpoint.CheckpointManager(f\"{ckpt_dir}/ckpts/\", orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler()), mgr_options)\n",
    "\n",
    "# restore_args = flax.training.orbax_utils.restore_args_from_target(state, mesh=None)\n",
    "# restored_state = ckpt_mgr.restore(ckpt_mgr.latest_step(), items=state, restore_kwargs={'restore_args': restore_args})\n",
    "\n",
    "# if state is restored_state:\n",
    "#     raise FileNotFoundError(f\"Did not load checkpoint correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ef16248-4b36-49ad-be66-fb1da53fae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = tf.io.gfile.glob(f\"/n/holyscratch01/iaifi_lab/smsharma/hubble_data/tfrecords_v5/*val*.tfrecord\")\n",
    "# ds = make_dataloader(files, batch_size=config.training.batch_size, seed=config.seed, split=\"val\", shuffle=True, caption_type=\"summary\")\n",
    "\n",
    "# image_feat_stack, captions_feat_stack, images_stack, captions_stack = get_features_ds(restored_state, ds, use_sum1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfdbc845-75a4-4743-8049-e6db1af6036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import orbax\n",
    "\n",
    "# run_name = \"cool-aardvark-77\"\n",
    "\n",
    "# ckpt_dir = \"{}/{}\".format(logging_dir, run_name)  # Load SLURM run\n",
    "\n",
    "# best_fn = lambda metrics: metrics[f\"val/loss\"]\n",
    "\n",
    "# mgr_options = orbax.checkpoint.CheckpointManagerOptions(step_prefix=f'step', best_fn=best_fn, best_mode='min', create=False)\n",
    "# ckpt_mgr = orbax.checkpoint.CheckpointManager(f\"{ckpt_dir}/ckpts/\", orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler()), mgr_options)\n",
    "\n",
    "# restore_args = flax.training.orbax_utils.restore_args_from_target(state, mesh=None)\n",
    "# restored_state = ckpt_mgr.restore(ckpt_mgr.latest_step(), items=state, restore_kwargs={'restore_args': restore_args})\n",
    "\n",
    "# if state is restored_state:\n",
    "#     raise FileNotFoundError(f\"Did not load checkpoint correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a162b59-65a9-4363-8421-bd87c69e40ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_feat_sum1_stack, captions_feat_sum1_stack, images_sum1_stack, captions_sum1_stack = get_features_ds(restored_state, ds, use_sum1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d40142f-99d3-4e12-baf1-456e47c6312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = np.linspace(-1, 1, 30)\n",
    "# lw = 2\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "\n",
    "# ax.hist(jax.vmap(cosine_similarity)(np.vstack(image_feat_stack), np.vstack(captions_feat_stack)[:]), histtype='step', label=\"Single-concept; fine-tuned on summarized abstract\", bins=bins, lw=lw)\n",
    "# ax.hist(jax.vmap(cosine_similarity)(np.vstack(image_feat_stack), jax.random.permutation(jax.random.PRNGKey(42), np.vstack(captions_feat_stack)[:])), histtype='step', bins=bins, lw=lw, ls='--', alpha=0.4, label=r\"---''---, shuffled\", color=cols_default[0])\n",
    "# ax.hist(jax.vmap(cosine_similarity)(np.vstack(image_feat_sum1_stack), np.vstack(captions_feat_sum1_stack)[:]), histtype='step', label=\"Single-concept; fine-tuned on single-concept\", bins=bins, lw=lw, color=cols_default[3])\n",
    "\n",
    "# ax.legend(frameon=True, framealpha=0.9)\n",
    "# ax.set_title(r\"\\textbf{Single-Concept Cosine Similarities}\", y=1.01)\n",
    "# ax.set_xlabel(\"$x_i\\cdot y_i$\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"../paper/plots/sim_summ1.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a331940-9759-4b2c-94b6-d2372ce36587",
   "metadata": {},
   "source": [
    "## Multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef189285-df3e-4360-98b1-80a8868bd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc(image_feat, captions_feat):\n",
    "    \n",
    "    roc_mat = np.matmul(image_feat, captions_feat.T)\n",
    "    \n",
    "    correct_indices = np.arange(roc_mat.shape[0])[:, None]\n",
    "    top_indices = np.argsort(roc_mat, axis=-1)\n",
    "    \n",
    "    accuracy_list = []\n",
    "    for k in np.arange(1, roc_mat.shape[0], 100):\n",
    "        correct_in_top_k = np.any(top_indices[:, -k:] == correct_indices, axis=-1)\n",
    "        accuracy = np.mean(correct_in_top_k.astype(np.float32))\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    return accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73e60d7d-6854-4c2a-8d61-28f4b7fd1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_labels = ['ancient-pine-88',\n",
    "             'fresh-jazz-84',\n",
    "              # 'cool-aardvark-77',\n",
    "              # 'spring-snowball-56',\n",
    "              ]\n",
    "\n",
    "run_legends = [\"Fine-tune (abstracts)\",\n",
    "              'Fine-tune (summaries)', \n",
    "               # 'Fine-tune (single concepts)',\n",
    "               # 'Train from scratch (summaries)',\n",
    "               ]\n",
    "\n",
    "data_type = [\"abstract\", \"summary\"] # , \"summary\", \"summary\"]\n",
    "use_sum1 = [False, False]  #, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "976345c4-c4a8-4d0e-9679-731b65312b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_eval_metric(text_embeds, image_embeds, k=[1, 5, 10, 20]):\n",
    "    \"\"\" Compute the top-k retrieval accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get shapes\n",
    "    bs = text_embeds.shape[0]\n",
    "    axis_size = jax.lax.psum(1, axis_name=\"batch\")\n",
    "\n",
    "    # Gather the embeddings from all devices\n",
    "    all_text_embeds = jax.lax.all_gather(text_embeds, axis_name=\"batch\").reshape(-1, text_embeds.shape[-1])\n",
    "    all_image_embeds = jax.lax.all_gather(image_embeds, axis_name=\"batch\").reshape(-1, image_embeds.shape[-1])\n",
    "\n",
    "    # Compute the full matrix of logitseval\n",
    "    all_logits = np.matmul(all_text_embeds, all_image_embeds.T)\n",
    "\n",
    "    # Compute the global top-k indices for the maximum k value\n",
    "    max_k = max(k)\n",
    "    top_k_indices = np.argsort(all_logits, axis=-1)[:, -max_k:]\n",
    "\n",
    "    # Compute the correct indices for each row\n",
    "    correct_indices = np.arange(bs * axis_size)[:, None]\n",
    "\n",
    "    metrics = {}\n",
    "    for current_k in k:\n",
    "        # Check if the correct image (diagonal) is in the current top-k for each text embedding\n",
    "        correct_in_top_k = np.any(top_k_indices[:, -current_k:] == correct_indices, axis=-1)\n",
    "        accuracy = np.mean(correct_in_top_k.astype(np.float32))\n",
    "        metrics[f\"top_{current_k}_accuracy\"] = accuracy\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "583d4643-8c1e-4749-be73-87422986c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from einops import rearrange\n",
    "import numpy as onp\n",
    "from models.losses import softmax_loss\n",
    "\n",
    "@partial(jax.pmap, axis_name=\"batch\")\n",
    "def get_features(state, input_ids, pixel_values, attention_mask):\n",
    "    # captions_feat = model.get_text_features(input_ids, attention_mask, params=state.params)\n",
    "    # images_feat = model.get_image_features(pixel_values, params=state.params)\n",
    "\n",
    "\n",
    "    outputs = state.apply_fn(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask, params=state.params)\n",
    "    captions_feat, images_feat = outputs[\"text_embeds\"], outputs[\"image_embeds\"]\n",
    "\n",
    "    outputs['logit_scale'] = state.params['logit_scale']\n",
    "    outputs['logit_bias'] = state.params.get('logit_bias', 0.)\n",
    "\n",
    "    loss = softmax_loss(outputs)\n",
    "    retrieval_metrics = retrieval_eval_metric(captions_feat, images_feat)\n",
    "\n",
    "    metrics = {\"loss\": loss}\n",
    "    for key, value in retrieval_metrics.items():\n",
    "        metrics[key] = value\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "def get_features_ds(state, ds, truncate=False):\n",
    "\n",
    "    batches = iter(ds)\n",
    "    \n",
    "    num_local_devices = jax.local_device_count()\n",
    "    replicate = flax.jax_utils.replicate\n",
    "    \n",
    "    total_batches = sum(1 for _ in ds) - 1\n",
    "    current_batch = 0\n",
    "\n",
    "    retrieval_eval_metrics = []\n",
    "\n",
    "    for (images, captions) in tqdm(batches, total=total_batches):\n",
    "        if current_batch == total_batches - 1:\n",
    "            break\n",
    "    \n",
    "        images = np.array(images)\n",
    "\n",
    "        if truncate:\n",
    "            captions = process_truncate_captions(captions, jax.random.PRNGKey(onp.random.randint(99999)), max_length_words=config.data.max_length_words)\n",
    "        else:\n",
    "            captions = captions.numpy().tolist()\n",
    "            captions = [c.decode('utf-8') for c in captions]\n",
    "\n",
    "        rng_eval = jax.random.PRNGKey(onp.random.randint(99999))\n",
    "        \n",
    "        # Rotations\n",
    "        rng_eval, _ = jax.random.split(rng_eval)\n",
    "        rotation_angles = jax.random.uniform(rng_eval, shape=(images.shape[0],)) * 2 * np.pi  # Angles in radians\n",
    "        images = jax.vmap(partial(rotate, mode='constant', cval=1.))(images, rotation_angles)\n",
    "        \n",
    "        # Flips\n",
    "        rng_eval, _ = jax.random.split(rng_eval)\n",
    "        images = jax.vmap(partial(random_flip_up_down, key=rng_eval))(image=images)\n",
    "\n",
    "        rng_eval, _ = jax.random.split(rng_eval)\n",
    "        images = jax.vmap(partial(random_flip_left_right, key=rng_eval))(image=images)\n",
    "\n",
    "        images = jax.vmap(random_crop, in_axes=(None,0,None))(rng_eval, images, (model.config.vision_config.image_size, model.config.vision_config.image_size, 3))\n",
    "\n",
    "        input = processor(text=captions, images=(images * 255.).astype(np.uint8), return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=77)\n",
    "    \n",
    "        batch = jax.tree_map(lambda x: np.split(x, num_local_devices, axis=0), input.data)\n",
    "        batch = jax.tree_map(lambda x: np.array(x, dtype=np.float32), batch)\n",
    "\n",
    "        metrics = get_features(replicate(state), np.array(batch[\"input_ids\"]), np.array(batch[\"pixel_values\"]), np.array(batch[\"attention_mask\"]))\n",
    "\n",
    "        retrieval_eval_metrics.append(metrics)\n",
    "        \n",
    "        current_batch += 1\n",
    "\n",
    "    return retrieval_eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "404f1ac0-3e64-4d6b-8c24-8a3ad8510453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 0/2 [00:00<?, ?it/s]\n",
      "  0%|                                       | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█                              | 1/31 [00:32<16:26, 32.88s/it]\u001b[A\n",
      "  6%|██                             | 2/31 [00:34<06:52, 14.22s/it]\u001b[A\n",
      " 10%|███                            | 3/31 [00:35<03:50,  8.24s/it]\u001b[A\n",
      " 13%|████                           | 4/31 [00:36<02:26,  5.44s/it]\u001b[A\n",
      " 16%|█████                          | 5/31 [00:37<01:41,  3.89s/it]\u001b[A\n",
      " 19%|██████                         | 6/31 [00:38<01:13,  2.95s/it]\u001b[A\n",
      " 23%|███████                        | 7/31 [00:39<00:56,  2.36s/it]\u001b[A\n",
      " 26%|████████                       | 8/31 [00:40<00:45,  1.98s/it]\u001b[A\n",
      " 29%|█████████                      | 9/31 [00:42<00:37,  1.72s/it]\u001b[A\n",
      " 32%|█████████▋                    | 10/31 [00:43<00:32,  1.54s/it]\u001b[A\n",
      " 35%|██████████▋                   | 11/31 [00:44<00:28,  1.41s/it]\u001b[A\n",
      " 39%|███████████▌                  | 12/31 [00:45<00:25,  1.33s/it]\u001b[A\n",
      " 42%|████████████▌                 | 13/31 [00:46<00:22,  1.26s/it]\u001b[A\n",
      " 45%|█████████████▌                | 14/31 [00:47<00:20,  1.22s/it]\u001b[A\n",
      " 48%|██████████████▌               | 15/31 [00:48<00:19,  1.19s/it]\u001b[A\n",
      " 52%|███████████████▍              | 16/31 [00:49<00:17,  1.17s/it]\u001b[A\n",
      " 55%|████████████████▍             | 17/31 [00:51<00:16,  1.16s/it]\u001b[A\n",
      " 58%|█████████████████▍            | 18/31 [00:52<00:14,  1.15s/it]\u001b[A\n",
      " 61%|██████████████████▍           | 19/31 [00:53<00:13,  1.15s/it]\u001b[A\n",
      " 65%|███████████████████▎          | 20/31 [00:54<00:12,  1.14s/it]\u001b[A\n",
      " 68%|████████████████████▎         | 21/31 [00:55<00:11,  1.14s/it]\u001b[A\n",
      " 71%|█████████████████████▎        | 22/31 [00:56<00:10,  1.14s/it]\u001b[A\n",
      " 74%|██████████████████████▎       | 23/31 [00:57<00:09,  1.13s/it]\u001b[A\n",
      " 77%|███████████████████████▏      | 24/31 [00:58<00:07,  1.13s/it]\u001b[A\n",
      " 81%|████████████████████████▏     | 25/31 [01:00<00:06,  1.13s/it]\u001b[A\n",
      " 84%|█████████████████████████▏    | 26/31 [01:01<00:05,  1.13s/it]\u001b[A\n",
      " 87%|██████████████████████████▏   | 27/31 [01:02<00:04,  1.13s/it]\u001b[A\n",
      " 90%|███████████████████████████   | 28/31 [01:03<00:03,  1.13s/it]\u001b[A\n",
      " 94%|████████████████████████████  | 29/31 [01:04<00:02,  1.14s/it]\u001b[A\n",
      " 97%|█████████████████████████████ | 30/31 [01:05<00:02,  2.19s/it]\u001b[A\n",
      " 50%|████████████████                | 1/2 [01:12<01:12, 72.48s/it]\n",
      "  0%|                                       | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█                              | 1/31 [00:01<00:32,  1.10s/it]\u001b[A\n",
      "  6%|██                             | 2/31 [00:02<00:35,  1.22s/it]\u001b[A\n",
      " 10%|███                            | 3/31 [00:03<00:32,  1.15s/it]\u001b[A\n",
      " 13%|████                           | 4/31 [00:04<00:30,  1.12s/it]\u001b[A\n",
      " 16%|█████                          | 5/31 [00:05<00:28,  1.10s/it]\u001b[A\n",
      " 19%|██████                         | 6/31 [00:06<00:27,  1.09s/it]\u001b[A\n",
      " 23%|███████                        | 7/31 [00:07<00:25,  1.08s/it]\u001b[A\n",
      " 26%|████████                       | 8/31 [00:08<00:24,  1.07s/it]\u001b[A\n",
      " 29%|█████████                      | 9/31 [00:09<00:23,  1.06s/it]\u001b[A\n",
      " 32%|█████████▋                    | 10/31 [00:10<00:22,  1.07s/it]\u001b[A\n",
      " 35%|██████████▋                   | 11/31 [00:11<00:21,  1.07s/it]\u001b[A\n",
      " 39%|███████████▌                  | 12/31 [00:13<00:20,  1.06s/it]\u001b[A\n",
      " 42%|████████████▌                 | 13/31 [00:14<00:19,  1.06s/it]\u001b[A\n",
      " 45%|█████████████▌                | 14/31 [00:15<00:17,  1.06s/it]\u001b[A\n",
      " 48%|██████████████▌               | 15/31 [00:16<00:16,  1.06s/it]\u001b[A\n",
      " 52%|███████████████▍              | 16/31 [00:17<00:15,  1.06s/it]\u001b[A\n",
      " 55%|████████████████▍             | 17/31 [00:18<00:14,  1.06s/it]\u001b[A\n",
      " 58%|█████████████████▍            | 18/31 [00:19<00:13,  1.06s/it]\u001b[A\n",
      " 61%|██████████████████▍           | 19/31 [00:20<00:12,  1.06s/it]\u001b[A\n",
      " 65%|███████████████████▎          | 20/31 [00:21<00:11,  1.06s/it]\u001b[A\n",
      " 68%|████████████████████▎         | 21/31 [00:22<00:10,  1.06s/it]\u001b[A\n",
      " 71%|█████████████████████▎        | 22/31 [00:23<00:09,  1.06s/it]\u001b[A\n",
      " 74%|██████████████████████▎       | 23/31 [00:24<00:08,  1.06s/it]\u001b[A\n",
      " 77%|███████████████████████▏      | 24/31 [00:25<00:07,  1.06s/it]\u001b[A\n",
      " 81%|████████████████████████▏     | 25/31 [00:26<00:06,  1.06s/it]\u001b[A\n",
      " 84%|█████████████████████████▏    | 26/31 [00:27<00:05,  1.06s/it]\u001b[A\n",
      " 87%|██████████████████████████▏   | 27/31 [00:28<00:04,  1.06s/it]\u001b[A\n",
      " 90%|███████████████████████████   | 28/31 [00:29<00:03,  1.06s/it]\u001b[A\n",
      " 94%|████████████████████████████  | 29/31 [00:31<00:02,  1.06s/it]\u001b[A\n",
      " 97%|█████████████████████████████ | 30/31 [00:32<00:01,  1.07s/it]\u001b[A\n",
      "100%|████████████████████████████████| 2/2 [01:49<00:00, 54.61s/it]\n"
     ]
    }
   ],
   "source": [
    "accuracy_lists = []\n",
    "for idx, run_name in enumerate(tqdm(run_labels[:])):\n",
    "\n",
    "    files = tf.io.gfile.glob(f\"/n/holyscratch01/iaifi_lab/smsharma/hubble_data/tfrecords_v5/*val*.tfrecord\")\n",
    "    ds = make_dataloader(files, batch_size=100, seed=324, split=\"val\", shuffle=True, caption_type=data_type[idx])\n",
    "    \n",
    "    ckpt_dir = \"{}/{}\".format(logging_dir, run_name)  # Load SLURM run\n",
    "    \n",
    "    best_fn = lambda metrics: metrics[f\"val/loss\"]\n",
    "    \n",
    "    mgr_options = orbax.checkpoint.CheckpointManagerOptions(step_prefix=f'step', best_fn=best_fn, best_mode='min', create=False)\n",
    "    ckpt_mgr = orbax.checkpoint.CheckpointManager(f\"{ckpt_dir}/ckpts/\", orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler()), mgr_options)\n",
    "    \n",
    "    restore_args = flax.training.orbax_utils.restore_args_from_target(state, mesh=None)\n",
    "    restored_state = ckpt_mgr.restore(ckpt_mgr.latest_step(), items=state, restore_kwargs={'restore_args': restore_args})\n",
    "    \n",
    "    if state is restored_state:\n",
    "        raise FileNotFoundError(f\"Did not load checkpoint correctly\")\n",
    "\n",
    "    retrieval_eval_metrics = get_features_ds(restored_state, ds, truncate=data_type[idx] == \"abstract\",)\n",
    "    accuracy_lists.append(retrieval_eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "592da267-3405-44ed-89a2-25cf47f74d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune (abstracts) {'val/loss': 3.271432, 'val/top_10_accuracy': 0.6563333, 'val/top_1_accuracy': 0.20866664, 'val/top_20_accuracy': 0.79166675, 'val/top_5_accuracy': 0.5076667}\n",
      "Fine-tune (summaries) {'val/loss': 3.3696606, 'val/top_10_accuracy': 0.6363334, 'val/top_1_accuracy': 0.22166663, 'val/top_20_accuracy': 0.74566656, 'val/top_5_accuracy': 0.49833333}\n"
     ]
    }
   ],
   "source": [
    "from flax.training import common_utils\n",
    "\n",
    "for idx, metric in enumerate(accuracy_lists):\n",
    "    val_metrics = common_utils.get_metrics(metric)\n",
    "    print(run_legends[idx], {f\"val/{k}\": v for k, v in jax.tree_map(lambda x: x.mean(), val_metrics).items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce36de59-cc1f-42fc-b663-6ac59802448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, accuracy_list in enumerate(accuracy_lists):\n",
    "#     plt.plot(np.linspace(0., 1., len(accuracy_list)), accuracy_list, label=run_legends[idx])\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], ls='--', alpha=0.5, color='grey')\n",
    "    \n",
    "# plt.legend()\n",
    "\n",
    "# plt.xlim(0, 1)\n",
    "# plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc03099-062c-4f78-9628-e2eae94f03b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:smsharma-multimodal-hubble] *",
   "language": "python",
   "name": "conda-env-smsharma-multimodal-hubble-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
