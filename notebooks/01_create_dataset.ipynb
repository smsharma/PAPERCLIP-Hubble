{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.abstract_utils import read_abstracts_file\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/abstracts.cat\"\n",
    "\n",
    "abstracts_df = read_abstracts_file(filename)\n",
    "\n",
    "# Drop rows with missing Cycle\n",
    "abstracts_df = abstracts_df.dropna(subset=['Cycle'])\n",
    "abstracts_df = abstracts_df[abstracts_df['Cycle'] != '']\n",
    "\n",
    "# Convert Cycle and ID to int\n",
    "abstracts_df['Cycle'] = abstracts_df['Cycle'].astype(int)\n",
    "abstracts_df['ID'] = abstracts_df['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prop. Type</th>\n",
       "      <th>Category</th>\n",
       "      <th>ID</th>\n",
       "      <th>Cycle</th>\n",
       "      <th>Title</th>\n",
       "      <th>PI</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENG/STIS/PAR</td>\n",
       "      <td></td>\n",
       "      <td>10000</td>\n",
       "      <td>12</td>\n",
       "      <td>STIS Pure Parallel Imaging Program: Cycle 12</td>\n",
       "      <td>Paul Goudfrooij</td>\n",
       "      <td>This is the default archival pure parallel pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GO</td>\n",
       "      <td>GALAXIES</td>\n",
       "      <td>10001</td>\n",
       "      <td>12</td>\n",
       "      <td>Locating Ultraluminous X-Ray Sources</td>\n",
       "      <td>Philip Kaaret</td>\n",
       "      <td>We propose to observe ultraluminous X-ray sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GO</td>\n",
       "      <td>AGN</td>\n",
       "      <td>10002</td>\n",
       "      <td>12</td>\n",
       "      <td>Detailed Study of X-ray Jets from a Complete S...</td>\n",
       "      <td>Eric Perlman</td>\n",
       "      <td>We propose deep followup HST and Chandra obse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GO</td>\n",
       "      <td>GALAXIES</td>\n",
       "      <td>10003</td>\n",
       "      <td>12</td>\n",
       "      <td>Deep Chandra and Hubble Observations of NGC469...</td>\n",
       "      <td>Craig Sarazin</td>\n",
       "      <td>We propose 4 new Chandra observations of NGC4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GO</td>\n",
       "      <td>AGN</td>\n",
       "      <td>10004</td>\n",
       "      <td>12</td>\n",
       "      <td>The Physics of Relativistic Jets: Chandra Imag...</td>\n",
       "      <td>F. Tavecchio</td>\n",
       "      <td>Extended jets have been a key target for Chan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Prop. Type  Category     ID  Cycle  \\\n",
       "0  ENG/STIS/PAR            10000     12   \n",
       "1            GO  GALAXIES  10001     12   \n",
       "2            GO       AGN  10002     12   \n",
       "3            GO  GALAXIES  10003     12   \n",
       "4            GO       AGN  10004     12   \n",
       "\n",
       "                                               Title               PI  \\\n",
       "0       STIS Pure Parallel Imaging Program: Cycle 12  Paul Goudfrooij   \n",
       "1               Locating Ultraluminous X-Ray Sources    Philip Kaaret   \n",
       "2  Detailed Study of X-ray Jets from a Complete S...     Eric Perlman   \n",
       "3  Deep Chandra and Hubble Observations of NGC469...    Craig Sarazin   \n",
       "4  The Physics of Relativistic Jets: Chandra Imag...     F. Tavecchio   \n",
       "\n",
       "                                            Abstract  \n",
       "0   This is the default archival pure parallel pr...  \n",
       "1   We propose to observe ultraluminous X-ray sou...  \n",
       "2   We propose deep followup HST and Chandra obse...  \n",
       "3   We propose 4 new Chandra observations of NGC4...  \n",
       "4   Extended jets have been a key target for Chan...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep specific Cycles\n",
    "\n",
    "cycle_min = 0\n",
    "cycle_max = 32\n",
    "\n",
    "abstracts_cycle_df = abstracts_df[(abstracts_df['Cycle'] >= cycle_min) & (abstracts_df['Cycle'] <= cycle_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract_ids = abstracts_cycle_df['ID'].values\n",
    "# abstracts_cycle_df['Cycle'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def remove_large_files(directory, size_limit=2*1024*1024):  # default size_limit is set to 2MB\n",
    "#     for foldername, subfolders, filenames in os.walk(directory):\n",
    "#         for filename in filenames:\n",
    "#             filepath = os.path.join(foldername, filename)\n",
    "#             if os.path.getsize(filepath) > size_limit:\n",
    "#                 try:\n",
    "#                     os.remove(filepath)\n",
    "#                     print(f\"Removed {filepath}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error removing {filepath}: {e}\")\n",
    "\n",
    "# directory_path = '../data/observations_v2/'\n",
    "# remove_large_files(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_filename = \"../data/summary_v1.csv\"\n",
    "\n",
    "summaries_df = pd.read_csv(summaries_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proposal_id</th>\n",
       "      <th>objects_phenomena</th>\n",
       "      <th>science_use_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14743</td>\n",
       "      <td>superluminous supernova, SN 2015bn, magnetar s...</td>\n",
       "      <td>late-time luminosity, proximity, sufficiently ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6507</td>\n",
       "      <td>spiral density waves, self-propagating star fo...</td>\n",
       "      <td>ages of the stellar populations, color gradien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14249</td>\n",
       "      <td>neutron stars, radio-pulsars, 166 Myr old rad...</td>\n",
       "      <td>long-term evolution of neutron stars, surface...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8781</td>\n",
       "      <td>blue star, luminous blue variable, major outb...</td>\n",
       "      <td>mass loss rate, temperature, luminosity, chem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10556</td>\n",
       "      <td>neutral hydrogen gas, Damped Lyman-alpha syste...</td>\n",
       "      <td>tracking neutral hydrogen gas, evolution of ne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   proposal_id                                  objects_phenomena  \\\n",
       "0        14743  superluminous supernova, SN 2015bn, magnetar s...   \n",
       "1         6507  spiral density waves, self-propagating star fo...   \n",
       "2        14249   neutron stars, radio-pulsars, 166 Myr old rad...   \n",
       "3         8781   blue star, luminous blue variable, major outb...   \n",
       "4        10556  neutral hydrogen gas, Damped Lyman-alpha syste...   \n",
       "\n",
       "                                   science_use_cases  \n",
       "0  late-time luminosity, proximity, sufficiently ...  \n",
       "1  ages of the stellar populations, color gradien...  \n",
       "2   long-term evolution of neutron stars, surface...  \n",
       "3   mass loss rate, temperature, luminosity, chem...  \n",
       "4  tracking neutral hydrogen gas, evolution of ne...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stars, color-magnitude diagrams (CMDs), Leo I dwarf spheroidal galaxy, WFPC2, ACS/WFC, bluer wavelengths, higher spatial resolution, larger field-of-view, metallicity distribution'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_df[summaries_df[\"proposal_id\"] == 10520][\"objects_phenomena\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(abstract, summary, image):\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Encode abstract to bytes\n",
    "    abstract_bytes = abstract.encode('utf-8')  \n",
    "    summary_bytes = summary.encode('utf-8')  \n",
    "\n",
    "    # Convert image to bytes\n",
    "    image_bytes = image.tobytes()\n",
    "\n",
    "    feature = {\n",
    "        'abstract': _bytes_feature(abstract_bytes),\n",
    "        'summary': _bytes_feature(summary_bytes),\n",
    "        'image': _bytes_feature(image_bytes),\n",
    "        'image_height': _int64_feature(height),\n",
    "        'image_width': _int64_feature(width)\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def write_tfrecord(abstracts, summaries, images, filename, metadata_file):\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for abstract, summary, image in zip(abstracts, summaries, images):\n",
    "            tf_example = serialize_example(abstract, summary, image)\n",
    "            writer.write(tf_example)\n",
    "    \n",
    "    # Write metadata to the auxiliary file\n",
    "    with open(metadata_file, 'a') as meta_file:\n",
    "        meta_file.write(f\"{filename}: {len(images)} images\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size is 3185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6597cb8e5b04202807e3b16436ac0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Images:   0%|          | 0/31859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing record 1\n",
      "Writing record 2\n",
      "Writing record 3\n",
      "Writing record 4\n",
      "Writing record 5\n",
      "Writing record 6\n",
      "Writing record 7\n",
      "Writing record 8\n",
      "Writing record 9\n",
      "Writing record 10\n",
      "Writing final record 11\n"
     ]
    }
   ],
   "source": [
    "def get_abstracts_and_images_and_write_tfrecords(data_folder, tfrecords_folder, abstracts_cycle_df, summaries_df, num_tfrecords, num_train_tfrecords):\n",
    "    \n",
    "    # Lists to store results\n",
    "    images_list = []\n",
    "    abstracts_list = []\n",
    "    summaries_list = []\n",
    "\n",
    "    # Collect directories that contain .jpg files and match the \"proposal_\" pattern, excluding unwanted directories\n",
    "    directories_with_images = [os.path.join(r, d)\n",
    "                               for r, dirs, files in os.walk(data_folder)\n",
    "                               for d in dirs\n",
    "                               if d.startswith(\"proposal_\") and not d.endswith('.ipynb_checkpoints')]\n",
    "\n",
    "    # Shuffle the list of directories\n",
    "    random.shuffle(directories_with_images)\n",
    "\n",
    "    # Get the total number of jpg files to be processed for the progress bar\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(data_folder) if any(file.endswith('.jpg') for file in files)])\n",
    "\n",
    "    # Calculate the chunk size\n",
    "    chunk_size = total_files // num_tfrecords if total_files >= num_tfrecords else 1\n",
    "\n",
    "    print(f\"Chunk size is {chunk_size}\")\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    pbar = tqdm(total=total_files, desc='Processing Images')\n",
    "\n",
    "    # Initialize the file number\n",
    "    file_num = 1\n",
    "    metadata_file = f'{tfrecords_folder}/metadata.txt'\n",
    "    \n",
    "    # Clear the metadata file if it exists\n",
    "    if os.path.exists(metadata_file):\n",
    "        os.remove(metadata_file)\n",
    "\n",
    "    # Create tfrecords folder if it doesn't exist\n",
    "    os.makedirs(tfrecords_folder, exist_ok=True)\n",
    "\n",
    "    # Walk through data folder\n",
    "    for directory in directories_with_images:\n",
    "        for file in os.listdir(directory):\n",
    "            if file.endswith(\".jpg\"):\n",
    "                \n",
    "                image_path = os.path.join(directory, file)\n",
    "                proposal_id = directory.split(\"proposal_\")[-1]  # Extract proposal id from the directory name\n",
    "                \n",
    "                # Extract abstract using the dataframe\n",
    "                abstract = abstracts_cycle_df[abstracts_cycle_df[\"ID\"] == int(proposal_id)][\"Abstract\"].values[0]\n",
    "                category = abstracts_cycle_df[abstracts_cycle_df[\"ID\"] == int(proposal_id)][\"Category\"].values[0]\n",
    "\n",
    "                objects_phenomena = summaries_df[summaries_df[\"proposal_id\"] == int(proposal_id)][\"objects_phenomena\"].values[0]\n",
    "                science_use_cases = summaries_df[summaries_df[\"proposal_id\"] == int(proposal_id)][\"science_use_cases\"].values[0]\n",
    "\n",
    "                summary = f\"{objects_phenomena}; {science_use_cases}\"\n",
    "\n",
    "                if category is not None:\n",
    "                    abstract = f\"Category: {category}. {abstract}\"\n",
    "                    \n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                image = np.array(image)\n",
    "\n",
    "                # Pad image to square\n",
    "                h, w, c = image.shape\n",
    "                max_dim = max(h, w)\n",
    "                padded_image = np.ones((max_dim, max_dim, c), dtype=np.uint8) * 255\n",
    "\n",
    "                # Calculate top and left padding\n",
    "                y_offset = (max_dim - h) // 2\n",
    "                x_offset = (max_dim - w) // 2\n",
    "\n",
    "                padded_image[y_offset : y_offset + h, x_offset : x_offset + w, :] = image\n",
    "\n",
    "                images_list.append(padded_image)\n",
    "                abstracts_list.append(abstract)\n",
    "                summaries_list.append(summary)\n",
    "                \n",
    "                pbar.update(1)  # Update the progress bar\n",
    "\n",
    "                # If the length of the lists reaches the chunk size, write to a TFRecord file\n",
    "                if len(images_list) >= chunk_size:\n",
    "                    print(f\"Writing record {file_num}\")\n",
    "\n",
    "                    # Either train or val\n",
    "                    if file_num <= num_train_tfrecords:\n",
    "                        filename = f\"{tfrecords_folder}/observations_train_{file_num}.tfrecord\"\n",
    "                    else:\n",
    "                        filename = f\"{tfrecords_folder}/observations_val_{file_num}.tfrecord\"\n",
    "                        \n",
    "                    write_tfrecord(abstracts_list, summaries_list, images_list, filename, metadata_file)\n",
    "                    \n",
    "                    # Reset the images and abstracts lists\n",
    "                    images_list = []\n",
    "                    abstracts_list = []\n",
    "                    summaries_list = []\n",
    "                    \n",
    "                    file_num += 1  # Increment the file number\n",
    "\n",
    "    # Write the remaining records to a TFRecord file if any\n",
    "    if images_list:\n",
    "        print(f\"Writing final record {file_num}\")\n",
    "        filename = f\"{tfrecords_folder}/observations_val_{file_num}.tfrecord\"\n",
    "        write_tfrecord(abstracts_list, summaries_list, images_list, filename, metadata_file)\n",
    "\n",
    "    pbar.close()  # Close the progress bar\n",
    "\n",
    "tfrecords_folder = \"../data/tfrecords_v4/\"\n",
    "data_folder = \"../data/observations_v1/\"\n",
    "num_tfrecords = 10\n",
    "num_train_tfrecords = 9\n",
    "\n",
    "get_abstracts_and_images_and_write_tfrecords(data_folder, tfrecords_folder, abstracts_cycle_df, summaries_df, num_tfrecords, num_train_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:smsharma-multimodal-hubble] *",
   "language": "python",
   "name": "conda-env-smsharma-multimodal-hubble-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
