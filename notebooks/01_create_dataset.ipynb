{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.abstract_utils import read_abstracts_file\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/abstracts.cat\"\n",
    "\n",
    "abstracts_df = read_abstracts_file(filename)\n",
    "\n",
    "# Drop rows with missing Cycle\n",
    "abstracts_df = abstracts_df.dropna(subset=['Cycle'])\n",
    "abstracts_df = abstracts_df[abstracts_df['Cycle'] != '']\n",
    "\n",
    "# Convert Cycle and ID to int\n",
    "abstracts_df['Cycle'] = abstracts_df['Cycle'].astype(int)\n",
    "abstracts_df['ID'] = abstracts_df['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prop. Type</th>\n",
       "      <th>Category</th>\n",
       "      <th>ID</th>\n",
       "      <th>Cycle</th>\n",
       "      <th>Title</th>\n",
       "      <th>PI</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENG/STIS/PAR</td>\n",
       "      <td></td>\n",
       "      <td>10000</td>\n",
       "      <td>12</td>\n",
       "      <td>STIS Pure Parallel Imaging Program: Cycle 12</td>\n",
       "      <td>Paul Goudfrooij</td>\n",
       "      <td>This is the default archival pure parallel pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GO</td>\n",
       "      <td>GALAXIES</td>\n",
       "      <td>10001</td>\n",
       "      <td>12</td>\n",
       "      <td>Locating Ultraluminous X-Ray Sources</td>\n",
       "      <td>Philip Kaaret</td>\n",
       "      <td>We propose to observe ultraluminous X-ray sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GO</td>\n",
       "      <td>AGN</td>\n",
       "      <td>10002</td>\n",
       "      <td>12</td>\n",
       "      <td>Detailed Study of X-ray Jets from a Complete S...</td>\n",
       "      <td>Eric Perlman</td>\n",
       "      <td>We propose deep followup HST and Chandra obse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GO</td>\n",
       "      <td>GALAXIES</td>\n",
       "      <td>10003</td>\n",
       "      <td>12</td>\n",
       "      <td>Deep Chandra and Hubble Observations of NGC469...</td>\n",
       "      <td>Craig Sarazin</td>\n",
       "      <td>We propose 4 new Chandra observations of NGC4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GO</td>\n",
       "      <td>AGN</td>\n",
       "      <td>10004</td>\n",
       "      <td>12</td>\n",
       "      <td>The Physics of Relativistic Jets: Chandra Imag...</td>\n",
       "      <td>F. Tavecchio</td>\n",
       "      <td>Extended jets have been a key target for Chan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Prop. Type  Category     ID  Cycle  \\\n",
       "0  ENG/STIS/PAR            10000     12   \n",
       "1            GO  GALAXIES  10001     12   \n",
       "2            GO       AGN  10002     12   \n",
       "3            GO  GALAXIES  10003     12   \n",
       "4            GO       AGN  10004     12   \n",
       "\n",
       "                                               Title               PI  \\\n",
       "0       STIS Pure Parallel Imaging Program: Cycle 12  Paul Goudfrooij   \n",
       "1               Locating Ultraluminous X-Ray Sources    Philip Kaaret   \n",
       "2  Detailed Study of X-ray Jets from a Complete S...     Eric Perlman   \n",
       "3  Deep Chandra and Hubble Observations of NGC469...    Craig Sarazin   \n",
       "4  The Physics of Relativistic Jets: Chandra Imag...     F. Tavecchio   \n",
       "\n",
       "                                            Abstract  \n",
       "0   This is the default archival pure parallel pr...  \n",
       "1   We propose to observe ultraluminous X-ray sou...  \n",
       "2   We propose deep followup HST and Chandra obse...  \n",
       "3   We propose 4 new Chandra observations of NGC4...  \n",
       "4   Extended jets have been a key target for Chan...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep specific Cycles\n",
    "\n",
    "cycle_min = 0\n",
    "cycle_max = 32\n",
    "\n",
    "abstracts_cycle_df = abstracts_df[(abstracts_df['Cycle'] >= cycle_min) & (abstracts_df['Cycle'] <= cycle_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract_ids = abstracts_cycle_df['ID'].values\n",
    "# abstracts_cycle_df['Cycle'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def remove_large_files(directory, size_limit=2*1024*1024):  # default size_limit is set to 2MB\n",
    "#     for foldername, subfolders, filenames in os.walk(directory):\n",
    "#         for filename in filenames:\n",
    "#             filepath = os.path.join(foldername, filename)\n",
    "#             if os.path.getsize(filepath) > size_limit:\n",
    "#                 try:\n",
    "#                     os.remove(filepath)\n",
    "#                     print(f\"Removed {filepath}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error removing {filepath}: {e}\")\n",
    "\n",
    "# directory_path = '../data/observations_v2/'\n",
    "# remove_large_files(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_filename = \"../data/summary_v2.csv\"\n",
    "\n",
    "summaries_df = pd.read_csv(summaries_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proposal_id</th>\n",
       "      <th>objects_phenomena</th>\n",
       "      <th>science_use_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14743</td>\n",
       "      <td>Superluminous supernova SN 2015bn, magnetar, c...</td>\n",
       "      <td>Constrain explosion mechanism of SN 2015bn, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6507</td>\n",
       "      <td>spiral density waves, dwarf galaxies, irregula...</td>\n",
       "      <td>determine ages of stellar populations in NGC 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14249</td>\n",
       "      <td>neutron stars, pulsars, radio-pulsars, PSR J01...</td>\n",
       "      <td>understand long-term evolution of neutron star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8781</td>\n",
       "      <td>NGC 2363, extragalactic giant H II region, Lum...</td>\n",
       "      <td>monitor variations of LBV's mass loss rate, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10556</td>\n",
       "      <td>Damped Lyman-alpha systems, neutral hydrogen g...</td>\n",
       "      <td>Study evolution of neutral gas phase component...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   proposal_id                                  objects_phenomena  \\\n",
       "0        14743  Superluminous supernova SN 2015bn, magnetar, c...   \n",
       "1         6507  spiral density waves, dwarf galaxies, irregula...   \n",
       "2        14249  neutron stars, pulsars, radio-pulsars, PSR J01...   \n",
       "3         8781  NGC 2363, extragalactic giant H II region, Lum...   \n",
       "4        10556  Damped Lyman-alpha systems, neutral hydrogen g...   \n",
       "\n",
       "                                   science_use_cases  \n",
       "0  Constrain explosion mechanism of SN 2015bn, di...  \n",
       "1  determine ages of stellar populations in NGC 3...  \n",
       "2  understand long-term evolution of neutron star...  \n",
       "3  monitor variations of LBV's mass loss rate, te...  \n",
       "4  Study evolution of neutral gas phase component...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leo I dSph, color-magnitude diagrams, stellar populations, stellar evolutionary models, spectroscopy'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_df[summaries_df[\"proposal_id\"] == 10520][\"objects_phenomena\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-01 16:06:14.161208: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-01 16:06:14.162657: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-01 16:06:14.385899: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(abstract, summary, image):\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Encode abstract to bytes\n",
    "    abstract_bytes = abstract.encode('utf-8')  \n",
    "    summary_bytes = summary.encode('utf-8')  \n",
    "\n",
    "    # Convert image to bytes\n",
    "    image_bytes = image.tobytes()\n",
    "\n",
    "    feature = {\n",
    "        'abstract': _bytes_feature(abstract_bytes),\n",
    "        'summary': _bytes_feature(summary_bytes),\n",
    "        'image': _bytes_feature(image_bytes),\n",
    "        'image_height': _int64_feature(height),\n",
    "        'image_width': _int64_feature(width)\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def write_tfrecord(abstracts, summaries, images, filename, metadata_file):\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for abstract, summary, image in zip(abstracts, summaries, images):\n",
    "            tf_example = serialize_example(abstract, summary, image)\n",
    "            writer.write(tf_example)\n",
    "    \n",
    "    # Write metadata to the auxiliary file\n",
    "    with open(metadata_file, 'a') as meta_file:\n",
    "        meta_file.write(f\"{filename}: {len(images)} images\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# image = Image.open(\"../data/observations_v1/proposal_10001/hst_10001_01_acs_wfc_total_j8vo01_drc.jpg\").convert(\"RGB\")\n",
    "# image = np.array(image)\n",
    "\n",
    "# # Pad image to square\n",
    "# h, w, c = image.shape\n",
    "# max_dim = max(h, w)\n",
    "# padded_image = np.ones((max_dim, max_dim, c), dtype=np.uint8) * 255\n",
    "\n",
    "# # Calculate top and left padding\n",
    "# y_offset = (max_dim - h) // 2\n",
    "# x_offset = (max_dim - w) // 2\n",
    "\n",
    "# padded_image[y_offset : y_offset + h, x_offset : x_offset + w, :] = image\n",
    "\n",
    "# plt.imshow(padded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size is 3185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39af00b4ac84ce39bfb38fa212b9056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Images:   0%|          | 0/31859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing record 1\n",
      "Writing record 2\n",
      "Writing record 3\n",
      "Writing record 4\n",
      "Writing record 5\n",
      "Writing record 6\n",
      "Writing record 7\n",
      "Writing record 8\n",
      "Writing record 9\n",
      "Writing record 10\n",
      "Writing final record 11\n"
     ]
    }
   ],
   "source": [
    "def get_abstracts_and_images_and_write_tfrecords(data_folder, tfrecords_folder, abstracts_cycle_df, summaries_df, num_tfrecords, num_train_tfrecords):\n",
    "    \n",
    "    # Lists to store results\n",
    "    images_list = []\n",
    "    abstracts_list = []\n",
    "    summaries_list = []\n",
    "\n",
    "    # Collect directories that contain .jpg files and match the \"proposal_\" pattern, excluding unwanted directories\n",
    "    directories_with_images = [os.path.join(r, d)\n",
    "                               for r, dirs, files in os.walk(data_folder)\n",
    "                               for d in dirs\n",
    "                               if d.startswith(\"proposal_\") and not d.endswith('.ipynb_checkpoints')]\n",
    "\n",
    "    # Shuffle the list of directories\n",
    "    random.shuffle(directories_with_images)\n",
    "\n",
    "    # Get the total number of jpg files to be processed for the progress bar\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(data_folder) if any(file.endswith('.jpg') for file in files)])\n",
    "\n",
    "    # Calculate the chunk size\n",
    "    chunk_size = total_files // num_tfrecords if total_files >= num_tfrecords else 1\n",
    "\n",
    "    print(f\"Chunk size is {chunk_size}\")\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    pbar = tqdm(total=total_files, desc='Processing Images')\n",
    "\n",
    "    # Initialize the file number\n",
    "    file_num = 1\n",
    "    metadata_file = f'{tfrecords_folder}/metadata.txt'\n",
    "    \n",
    "    # Clear the metadata file if it exists\n",
    "    if os.path.exists(metadata_file):\n",
    "        os.remove(metadata_file)\n",
    "\n",
    "    # Create tfrecords folder if it doesn't exist\n",
    "    os.makedirs(tfrecords_folder, exist_ok=True)\n",
    "\n",
    "    # Walk through data folder\n",
    "    for directory in directories_with_images:\n",
    "        for file in os.listdir(directory):\n",
    "            if file.endswith(\".jpg\"):\n",
    "                \n",
    "                image_path = os.path.join(directory, file)\n",
    "                proposal_id = directory.split(\"proposal_\")[-1]  # Extract proposal id from the directory name\n",
    "                \n",
    "                # Extract abstract using the dataframe\n",
    "                abstract = abstracts_cycle_df[abstracts_cycle_df[\"ID\"] == int(proposal_id)][\"Abstract\"].values[0]\n",
    "                category = abstracts_cycle_df[abstracts_cycle_df[\"ID\"] == int(proposal_id)][\"Category\"].values[0]\n",
    "\n",
    "                objects_phenomena = summaries_df[summaries_df[\"proposal_id\"] == int(proposal_id)][\"objects_phenomena\"].values[0]\n",
    "                science_use_cases = summaries_df[summaries_df[\"proposal_id\"] == int(proposal_id)][\"science_use_cases\"].values[0]\n",
    "\n",
    "                summary = f\"{objects_phenomena}; {science_use_cases}\"\n",
    "\n",
    "                if category is not None:\n",
    "                    abstract = f\"Category: {category}. {abstract}\"\n",
    "                    \n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                image = np.array(image)\n",
    "\n",
    "                # Pad image to square\n",
    "                h, w, c = image.shape\n",
    "                max_dim = max(h, w)\n",
    "                padded_image = np.ones((max_dim, max_dim, c), dtype=np.uint8) * 255\n",
    "\n",
    "                # Calculate top and left padding\n",
    "                y_offset = (max_dim - h) // 2\n",
    "                x_offset = (max_dim - w) // 2\n",
    "\n",
    "                padded_image[y_offset : y_offset + h, x_offset : x_offset + w, :] = image\n",
    "                \n",
    "                images_list.append(padded_image)\n",
    "                abstracts_list.append(abstract)\n",
    "                summaries_list.append(summary)\n",
    "                \n",
    "                pbar.update(1)  # Update the progress bar\n",
    "\n",
    "                # If the length of the lists reaches the chunk size, write to a TFRecord file\n",
    "                if len(images_list) >= chunk_size:\n",
    "                    print(f\"Writing record {file_num}\")\n",
    "\n",
    "                    # Either train or val\n",
    "                    if file_num <= num_train_tfrecords:\n",
    "                        filename = f\"{tfrecords_folder}/observations_train_{file_num}.tfrecord\"\n",
    "                    else:\n",
    "                        filename = f\"{tfrecords_folder}/observations_val_{file_num}.tfrecord\"\n",
    "                        \n",
    "                    write_tfrecord(abstracts_list, summaries_list, images_list, filename, metadata_file)\n",
    "                    \n",
    "                    # Reset the images and abstracts lists\n",
    "                    images_list = []\n",
    "                    abstracts_list = []\n",
    "                    summaries_list = []\n",
    "                    \n",
    "                    file_num += 1  # Increment the file number\n",
    "\n",
    "    # Write the remaining records to a TFRecord file if any\n",
    "    if images_list:\n",
    "        print(f\"Writing final record {file_num}\")\n",
    "        filename = f\"{tfrecords_folder}/observations_val_{file_num}.tfrecord\"\n",
    "        write_tfrecord(abstracts_list, summaries_list, images_list, filename, metadata_file)\n",
    "\n",
    "    pbar.close()  # Close the progress bar\n",
    "\n",
    "tfrecords_folder = \"/n/holyscratch01/iaifi_lab/smsharma/hubble_data/tfrecords_v5/\"\n",
    "data_folder = \"../data/observations_v1/\"\n",
    "num_tfrecords = 10\n",
    "num_train_tfrecords = 9\n",
    "\n",
    "get_abstracts_and_images_and_write_tfrecords(data_folder, tfrecords_folder, abstracts_cycle_df, summaries_df, num_tfrecords, num_train_tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images and abstracts for paper; Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "data_folder = \"../data/observations_v1/\"\n",
    "save_dir = \"../paper/plots/data/\"\n",
    "\n",
    "# Collect directories that contain .jpg files and match the \"proposal_\" pattern, excluding unwanted directories\n",
    "directories_with_images = [os.path.join(r, d)\n",
    "                           for r, dirs, files in os.walk(data_folder)\n",
    "                           for d in dirs\n",
    "                           if d.startswith(\"proposal_\") and not d.endswith('.ipynb_checkpoints')]\n",
    "random.seed(99999)\n",
    "random.seed(1e9 + 5345)\n",
    "random.shuffle(directories_with_images)\n",
    "\n",
    "# fig, ax = plt.subplots(1, 4, figsize=(20,4))\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for directory in directories_with_images[:4]:\n",
    "    proposal_id = directory.split(\"proposal_\")[-1]  # Extract proposal id from the directory name\n",
    "\n",
    "    for file in os.listdir(directory)[0:1]:\n",
    "        if file.endswith(\".jpg\"):      \n",
    "            image_path = os.path.join(directory, file)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = np.array(image)\n",
    "\n",
    "            # Pad image to square\n",
    "            h, w, c = image.shape\n",
    "            max_dim = max(h, w)\n",
    "            padded_image = np.ones((max_dim, max_dim, c), dtype=np.uint8) * 255\n",
    "\n",
    "            # Calculate top and left padding\n",
    "            y_offset = (max_dim - h) // 2\n",
    "            x_offset = (max_dim - w) // 2\n",
    "\n",
    "            padded_image[y_offset : y_offset + h, x_offset : x_offset + w, :] = image\n",
    "\n",
    "            # ax[ii].imshow(padded_image)\n",
    "\n",
    "            # Extract abstract using the dataframe\n",
    "            abstract = abstracts_cycle_df[abstracts_cycle_df[\"ID\"] == int(proposal_id)][\"Abstract\"].values[0]\n",
    "            category = abstracts_cycle_df[abstracts_cycle_df[\"ID\"] == int(proposal_id)][\"Category\"].values[0]\n",
    "\n",
    "            objects_phenomena = summaries_df[summaries_df[\"proposal_id\"] == int(proposal_id)][\"objects_phenomena\"].values[0]\n",
    "            science_use_cases = summaries_df[summaries_df[\"proposal_id\"] == int(proposal_id)][\"science_use_cases\"].values[0]\n",
    "\n",
    "            summary = f\"{objects_phenomena}; {science_use_cases}\"\n",
    "\n",
    "            with open(save_dir + f\"/sum_{ii}.txt\", 'w') as file:\n",
    "                file.write(summary)\n",
    "\n",
    "            with open(save_dir + f\"/abs_{ii}.txt\", 'w') as file:\n",
    "                file.write(abstract)\n",
    "\n",
    "            with open(save_dir + f\"/id_{ii}.txt\", 'w') as file:\n",
    "                file.write(proposal_id)\n",
    "\n",
    "            with open(save_dir + f\"/cycle_{ii}.txt\", 'w') as file:\n",
    "                file.write(str(abstracts_cycle_df[abstracts_cycle_df[\"ID\"] == int(proposal_id)][\"Cycle\"].values[0]))\n",
    "\n",
    "            plt.figure(figsize=(8,8))\n",
    "            plt.imshow(padded_image)\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_dir + f\"/img_{ii}.pdf\")\n",
    "            plt.close()\n",
    "\n",
    "            ii += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstracts and summaries; Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:32: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:32: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/tmp/ipykernel_144846/2593457559.py:32: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if category is not None and category.strip(\" \") is not \"\":\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"../data/observations_v1/\"\n",
    "save_dir = \"../paper/plots/data/\"\n",
    "\n",
    "# Collect directories that contain .jpg files and match the \"proposal_\" pattern, excluding unwanted directories\n",
    "directories_with_images = [os.path.join(r, d)\n",
    "                           for r, dirs, files in os.walk(data_folder)\n",
    "                           for d in dirs\n",
    "                           if d.startswith(\"proposal_\") and not d.endswith('.ipynb_checkpoints')]\n",
    "random.seed(99999)\n",
    "random.seed(1e9 + 5345)\n",
    "random.shuffle(directories_with_images)\n",
    "\n",
    "ii = 0\n",
    "n_max_words = 55\n",
    "\n",
    "for directory in directories_with_images[:4]:\n",
    "    proposal_id = directory.split(\"proposal_\")[-1]  # Extract proposal id from the directory name\n",
    "\n",
    "    for file in os.listdir(directory)[0:1]:\n",
    "        if file.endswith(\".jpg\"):      \n",
    "            image_path = os.path.join(directory, file)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = np.array(image)\n",
    "\n",
    "            # Extract abstract using the dataframe\n",
    "            abstract = abstracts_cycle_df[abstracts_cycle_df[\"ID\"] == int(proposal_id)][\"Abstract\"].values[0]\n",
    "            category = abstracts_cycle_df[abstracts_cycle_df[\"ID\"] == int(proposal_id)][\"Category\"].values[0]\n",
    "\n",
    "            objects_phenomena = summaries_df[summaries_df[\"proposal_id\"] == int(proposal_id)][\"objects_phenomena\"].values[0]\n",
    "            science_use_cases = summaries_df[summaries_df[\"proposal_id\"] == int(proposal_id)][\"science_use_cases\"].values[0]\n",
    "\n",
    "            if category is not None and category.strip(\" \") is not \"\":\n",
    "                abstract = f\"Category: {category}. {abstract}\"\n",
    "\n",
    "            with open(save_dir + f\"/sci1_{ii}.txt\", 'w') as file:\n",
    "                file.write(science_use_cases)\n",
    "\n",
    "            with open(save_dir + f\"/obj1_{ii}.txt\", 'w') as file:\n",
    "                file.write(objects_phenomena)\n",
    "\n",
    "            with open(save_dir + f\"/id1_{ii}.txt\", 'w') as file:\n",
    "                file.write(proposal_id)\n",
    "\n",
    "            with open(save_dir + f\"/abs1_{ii}.txt\", 'w') as file:\n",
    "                file.write(' '.join(abstract.split()[:n_max_words]) + '...')\n",
    "\n",
    "            with open(save_dir + f\"/cycle1_{ii}.txt\", 'w') as file:\n",
    "                file.write(str(abstracts_cycle_df[abstracts_cycle_df[\"ID\"] == int(proposal_id)][\"Cycle\"].values[0]))\n",
    "\n",
    "            ii += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4438"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:smsharma-multimodal-hubble] *",
   "language": "python",
   "name": "conda-env-smsharma-multimodal-hubble-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
