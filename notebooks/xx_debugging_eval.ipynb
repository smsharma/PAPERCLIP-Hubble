{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a099dd4-cd60-4df7-9fbf-2136e00cfa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 18:01:07.414616: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-12 18:01:07.414658: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-12 18:01:07.415937: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from transformers import FlaxCLIPModel, AutoProcessor, AutoTokenizer\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca87e85f-3e24-4e66-ab8a-1cf4af3c44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlaxCLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc3a1bfa-d17d-49c5-a009-eb7b82b56120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "from matplotlib import cm\n",
    "cmap = matplotlib.colormaps.get_cmap('viridis_r')\n",
    "\n",
    "# Ignore warning\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.MatplotlibDeprecationWarning)\n",
    "\n",
    "# Get plot params\n",
    "\n",
    "from plot_params import params\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "# Set default colors to load at will\n",
    "cols_default = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d83087c-1986-44ff-8953-694c3a2a31fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax \n",
    "from flax.training import train_state\n",
    "import flax\n",
    "import orbax\n",
    "\n",
    "replicate = flax.jax_utils.replicate\n",
    "unreplicate = flax.jax_utils.unreplicate\n",
    "\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=1e-4,\n",
    "    warmup_steps=5_000,\n",
    "    decay_steps=100_000,\n",
    ")\n",
    "\n",
    "tx = optax.adamw(learning_rate=schedule, weight_decay=1e-4)\n",
    "state = train_state.TrainState.create(apply_fn=model.__call__, params=model.params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a42730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from ml_collections.config_dict import ConfigDict\n",
    "\n",
    "logging_dir = '../logging/proposals/'\n",
    "run_name = 'ancient-pine-88'\n",
    "\n",
    "config_file = \"{}/{}/config.yaml\".format(logging_dir, run_name)\n",
    "\n",
    "with open(config_file, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "config = ConfigDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37674e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ckpt_dir = \"{}/{}\".format(logging_dir, run_name)  # Load SLURM run\n",
    "\n",
    "# best_fn = lambda metrics: metrics[f\"val/top_10_accuracy\"]\n",
    "\n",
    "# mgr_options = orbax.checkpoint.CheckpointManagerOptions(step_prefix=f'step', best_fn=best_fn, best_mode='min', create=False)\n",
    "# ckpt_mgr = orbax.checkpoint.CheckpointManager(f\"{ckpt_dir}/ckpts/\", orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler()), mgr_options)\n",
    "\n",
    "# restore_args = flax.training.orbax_utils.restore_args_from_target(state, mesh=None)\n",
    "# restored_state = ckpt_mgr.restore(ckpt_mgr.latest_step(), items=state, restore_kwargs={'restore_args': restore_args})\n",
    "\n",
    "# if state is restored_state:\n",
    "#     raise FileNotFoundError(f\"Did not load checkpoint correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a331940-9759-4b2c-94b6-d2372ce36587",
   "metadata": {},
   "source": [
    "## Multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c783dec2-b2f3-454c-8782-576da5bf6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from utils.dataset_utils import make_dataloader, create_input_iter\n",
    "from utils.text_utils import process_truncate_captions\n",
    "from dm_pix import center_crop, random_crop, rotate, random_flip_up_down, random_flip_left_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73e60d7d-6854-4c2a-8d61-28f4b7fd1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_labels = ['ancient-pine-88',]\n",
    "run_legends = [\"Fine-tune (abstracts)\"]\n",
    "\n",
    "data_type = [\"abstract\"]\n",
    "use_sum1 = [False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "976345c4-c4a8-4d0e-9679-731b65312b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_eval_metric(text_embeds, image_embeds, k=[1, 5, 10, 20]):\n",
    "    \"\"\" Compute the top-k retrieval accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get shapes\n",
    "    bs = text_embeds.shape[0]\n",
    "    axis_size = jax.lax.psum(1, axis_name=\"batch\")\n",
    "\n",
    "    # Gather the embeddings from all devices\n",
    "    all_text_embeds = jax.lax.all_gather(text_embeds, axis_name=\"batch\").reshape(-1, text_embeds.shape[-1])\n",
    "    all_image_embeds = jax.lax.all_gather(image_embeds, axis_name=\"batch\").reshape(-1, image_embeds.shape[-1])\n",
    "\n",
    "    # Compute the full matrix of logitseval\n",
    "    all_logits = np.matmul(all_text_embeds, all_image_embeds.T)\n",
    "\n",
    "    # Compute the global top-k indices for the maximum k value\n",
    "    max_k = max(k)\n",
    "    top_k_indices = np.argsort(all_logits, axis=-1)[:, -max_k:]\n",
    "\n",
    "    # Compute the correct indices for each row\n",
    "    correct_indices = np.arange(bs * axis_size)[:, None]\n",
    "\n",
    "    metrics = {}\n",
    "    for current_k in k:\n",
    "        # Check if the correct image (diagonal) is in the current top-k for each text embedding\n",
    "        correct_in_top_k = np.any(top_k_indices[:, -current_k:] == correct_indices, axis=-1)\n",
    "        accuracy = np.mean(correct_in_top_k.astype(np.float32))\n",
    "        metrics[f\"top_{current_k}_accuracy\"] = accuracy\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "583d4643-8c1e-4749-be73-87422986c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from einops import rearrange\n",
    "import numpy as onp\n",
    "from models.losses import softmax_loss\n",
    "\n",
    "@partial(jax.pmap, axis_name=\"batch\")\n",
    "def get_features(state, input_ids, pixel_values, attention_mask):\n",
    "\n",
    "    # captions_feat = model.get_text_features(input_ids, attention_mask, params=state.params)\n",
    "    # images_feat = model.get_image_features(pixel_values, params=state.params)\n",
    "\n",
    "    outputs = state.apply_fn(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask, params=state.params)\n",
    "    captions_feat, images_feat = outputs[\"text_embeds\"], outputs[\"image_embeds\"]\n",
    "\n",
    "    outputs['logit_scale'] = state.params['logit_scale']\n",
    "    outputs['logit_bias'] = state.params.get('logit_bias', 0.)\n",
    "\n",
    "    loss = softmax_loss(outputs)\n",
    "    retrieval_metrics = retrieval_eval_metric(captions_feat, images_feat)\n",
    "\n",
    "    metrics = {\"loss\": loss}\n",
    "    for key, value in retrieval_metrics.items():\n",
    "        metrics[key] = value\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "def get_features_ds(state, ds, truncate=False):\n",
    "\n",
    "    batches = iter(ds)\n",
    "    \n",
    "    num_local_devices = jax.local_device_count()\n",
    "    replicate = flax.jax_utils.replicate\n",
    "    \n",
    "    total_batches = sum(1 for _ in ds) - 1\n",
    "    current_batch = 0\n",
    "\n",
    "    retrieval_eval_metrics = []\n",
    "\n",
    "    for (images, captions) in tqdm(batches, total=total_batches):\n",
    "        if current_batch == total_batches - 1:\n",
    "            break\n",
    "    \n",
    "        images = np.array(images)\n",
    "\n",
    "        if truncate:\n",
    "            captions = process_truncate_captions(captions, jax.random.PRNGKey(onp.random.randint(99999)), max_length_words=config.data.max_length_words)\n",
    "        else:\n",
    "            captions = captions.numpy().tolist()\n",
    "            captions = [c.decode('utf-8') for c in captions]\n",
    "\n",
    "        rng_eval = jax.random.PRNGKey(onp.random.randint(99999))\n",
    "        \n",
    "        # Rotations\n",
    "        rng_eval, _ = jax.random.split(rng_eval)\n",
    "        rotation_angles = jax.random.uniform(rng_eval, shape=(images.shape[0],)) * 2 * np.pi  # Angles in radians\n",
    "        images = jax.vmap(partial(rotate, mode='constant', cval=1.))(images, rotation_angles)\n",
    "        \n",
    "        # Flips\n",
    "        rng_eval, _ = jax.random.split(rng_eval)\n",
    "        images = jax.vmap(partial(random_flip_up_down, key=rng_eval))(image=images)\n",
    "\n",
    "        rng_eval, _ = jax.random.split(rng_eval)\n",
    "        images = jax.vmap(partial(random_flip_left_right, key=rng_eval))(image=images)\n",
    "\n",
    "        images = jax.vmap(random_crop, in_axes=(None,0,None))(rng_eval, images, (model.config.vision_config.image_size, model.config.vision_config.image_size, 3))\n",
    "\n",
    "        input = processor(text=captions, images=(images * 255.).astype(np.uint8), return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=77)\n",
    "    \n",
    "        batch = jax.tree_map(lambda x: np.split(x, num_local_devices, axis=0), input.data)\n",
    "        batch = jax.tree_map(lambda x: np.array(x, dtype=np.float32), batch)\n",
    "\n",
    "        metrics = get_features(replicate(state), np.array(batch[\"input_ids\"]), np.array(batch[\"pixel_values\"]), np.array(batch[\"attention_mask\"]))\n",
    "\n",
    "        retrieval_eval_metrics.append(metrics)\n",
    "        \n",
    "        current_batch += 1\n",
    "\n",
    "    return retrieval_eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404f1ac0-3e64-4d6b-8c24-8a3ad8510453",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_lists = []\n",
    "for idx, run_name in enumerate(tqdm(run_labels[:])):\n",
    "\n",
    "    files = tf.io.gfile.glob(f\"/n/holyscratch01/iaifi_lab/smsharma/hubble_data/tfrecords_v5/*val*.tfrecord\")\n",
    "    ds = make_dataloader(files, batch_size=100, seed=42, split=\"val\", shuffle=True, caption_type=data_type[idx])\n",
    "    \n",
    "    ckpt_dir = \"{}/{}\".format(logging_dir, run_name)  # Load SLURM run\n",
    "    \n",
    "    best_fn = lambda metrics: metrics[f\"val/loss\"]\n",
    "    \n",
    "    mgr_options = orbax.checkpoint.CheckpointManagerOptions(step_prefix=f'step', best_fn=best_fn, best_mode='min', create=False)\n",
    "    ckpt_mgr = orbax.checkpoint.CheckpointManager(f\"{ckpt_dir}/ckpts/\", orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler()), mgr_options)\n",
    "    \n",
    "    restore_args = flax.training.orbax_utils.restore_args_from_target(state, mesh=None)\n",
    "    restored_state = ckpt_mgr.restore(ckpt_mgr.latest_step(), items=state, restore_kwargs={'restore_args': restore_args})\n",
    "    \n",
    "    if state is restored_state:\n",
    "        raise FileNotFoundError(f\"Did not load checkpoint correctly\")\n",
    "\n",
    "    retrieval_eval_metrics = get_features_ds(restored_state, ds, truncate=data_type[idx] == \"abstract\",)\n",
    "    accuracy_lists.append(retrieval_eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "592da267-3405-44ed-89a2-25cf47f74d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune (abstracts) {'val/loss': 3.2589102, 'val/top_10_accuracy': 0.66666675, 'val/top_1_accuracy': 0.20033331, 'val/top_20_accuracy': 0.79533327, 'val/top_5_accuracy': 0.522}\n",
      "Fine-tune (summaries) {'val/loss': 3.3698084, 'val/top_10_accuracy': 0.625, 'val/top_1_accuracy': 0.22099997, 'val/top_20_accuracy': 0.74966675, 'val/top_5_accuracy': 0.4993333}\n"
     ]
    }
   ],
   "source": [
    "from flax.training import common_utils\n",
    "\n",
    "for idx, metric in enumerate(accuracy_lists):\n",
    "    val_metrics = common_utils.get_metrics(metric)\n",
    "    print(run_legends[idx], {f\"val/{k}\": v for k, v in jax.tree_map(lambda x: x.mean(), val_metrics).items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b36156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
